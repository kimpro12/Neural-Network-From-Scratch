{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "collapsed_sections": [
        "McWZYE_QqyUe"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Code using numpy"
      ],
      "metadata": {
        "id": "572ieRzbURZk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import libraries"
      ],
      "metadata": {
        "id": "rWMaCqROUdNL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "3rhiL_plUUTl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import data"
      ],
      "metadata": {
        "id": "6ZpjPJENUfTd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "df = pd.read_csv('/content/drive/MyDrive/train.csv')\n",
        "print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ERQxjKeUg5r",
        "outputId": "dfa25b88-4833-435e-cf0b-3352bedc29a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "   v.id  on road old  on road now  years      km  rating  condition  economy  \\\n",
            "0     1       535651       798186      3   78945       1          2       14   \n",
            "1     2       591911       861056      6  117220       5          9        9   \n",
            "2     3       686990       770762      2  132538       2          8       15   \n",
            "3     4       573999       722381      4  101065       4          3       11   \n",
            "4     5       691388       811335      6   61559       3          9       12   \n",
            "\n",
            "   top speed  hp  torque  current price  \n",
            "0        177  73     123       351318.0  \n",
            "1        148  74      95       285001.5  \n",
            "2        181  53      97       215386.0  \n",
            "3        197  54     116       244295.5  \n",
            "4        160  53     105       531114.5  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = df.drop(df.columns[0], axis = 1).to_numpy()\n",
        "X = data[:, :-1]\n",
        "y = data[:, -1]\n",
        "y = np.expand_dims(y, axis = 1)\n",
        "X.shape, y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4KsD-a8YXfC4",
        "outputId": "5762aa18-5a18-4636-f191-78385e28bba3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1000, 10), (1000, 1))"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare the data"
      ],
      "metadata": {
        "id": "xi3BzS83Up-g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TRAIN_RATIO, VAL_RATIO = 0.8, 0.1\n",
        "BATCH_SIZE, SEED = 64, 123\n",
        "\n",
        "n = len(X)\n",
        "idx = np.arange(n)\n",
        "np.random.shuffle(idx)\n",
        "n_train = int(n * TRAIN_RATIO)\n",
        "n_val   = int(n * VAL_RATIO)\n",
        "n_test  = n - n_train - n_val\n",
        "\n",
        "idx_train = idx[:n_train]\n",
        "idx_val = idx[n_train:n_train + n_val]\n",
        "idx_test = idx[n_train + n_val:]\n",
        "\n",
        "X_train, y_train = X[idx_train], y[idx_train]\n",
        "X_val, y_val = X[idx_val], y[idx_val]\n",
        "X_test, y_test = X[idx_test], y[idx_test]"
      ],
      "metadata": {
        "id": "2Fk1aMFIUrVy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class StandardScaler:\n",
        "  def __init__(self):\n",
        "    self.mean_ = None\n",
        "    self.scale_ = None\n",
        "\n",
        "  def fit(self, X):\n",
        "    X = np.asarray(X, dtype = np.float64)\n",
        "    self.mean_ = np.mean(X, axis = 0)\n",
        "    self.scale_ = np.std(X, axis = 0, ddof = 0)\n",
        "    self.scale_ = np.where(self.scale_ == 0, 1.0, self.scale_)\n",
        "    return self\n",
        "\n",
        "  def transform(self, X):\n",
        "    if self.mean_ is None or self.scale_ is None:\n",
        "      raise ValueError(\"Bạn phải gọi fit() trước khi transform().\")\n",
        "\n",
        "    X = np.asarray(X, dtype=np.float64)\n",
        "    return (X - self.mean_) / self.scale_\n",
        "\n",
        "  def fit_transform(self, X):\n",
        "    return self.fit(X).transform(X)\n",
        "\n",
        "  def inverse_transform(self, X_scaled):\n",
        "    if self.mean_ is None or self.scale_ is None:\n",
        "      raise ValueError(\"Bạn phải gọi fit() trước khi inverse_transform().\")\n",
        "\n",
        "    X_scaled = np.asarray(X_scaled, dtype=np.float64)\n",
        "    return X_scaled * self.scale_ + self.mean_"
      ],
      "metadata": {
        "id": "H2jGPNb8ZVDd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaler_X = StandardScaler()\n",
        "X_train_s = scaler_X.fit_transform(X_train)\n",
        "X_val_s = scaler_X.transform(X_val)\n",
        "X_test_s = scaler_X.transform(X_test)\n",
        "\n",
        "scaler_y = StandardScaler()\n",
        "y_train_s = scaler_y.fit_transform(y_train)\n",
        "y_val_s = scaler_y.transform(y_val)\n",
        "y_test_s = scaler_y.transform(y_test)"
      ],
      "metadata": {
        "id": "20lpFfoic3k1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "6JQ4QWrUdt3j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initialize parameters"
      ],
      "metadata": {
        "id": "RG0JCHSixXHP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Thiết lập seed cho reproducibility\n",
        "np.random.seed(SEED)\n",
        "\n",
        "n_features = X_train_s.shape[1]\n",
        "h1, h2 = 64, 32  # hidden sizes\n",
        "out = 1          # regression\n",
        "\n",
        "# Khởi tạo He (tốt cho ReLU)\n",
        "def he_init(fan_in, fan_out):\n",
        "    return np.random.randn(fan_in, fan_out) * np.sqrt(2.0 / fan_in)\n",
        "\n",
        "W1 = he_init(n_features, h1); b1 = np.zeros((1, h1))\n",
        "W2 = he_init(h1, h2);        b2 = np.zeros((1, h2))\n",
        "W3 = he_init(h2, out);       b3 = np.zeros((1, out))\n",
        "\n",
        "# Dropout rates\n",
        "p_drop1 = 0.001   # sau ReLU1\n",
        "p_drop2 = 0.0005   # sau ReLU2\n",
        "USING_DROPOUT = False"
      ],
      "metadata": {
        "id": "NMne2VnCxeT3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Activation function and forward function"
      ],
      "metadata": {
        "id": "ba3jlPBwxhyT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def relu(x):        return np.maximum(0.0, x)\n",
        "def relu_grad(x):   return (x > 0).astype(x.dtype)  # d/dx ReLU\n",
        "\n",
        "def forward(Xb, using_dropout = False):\n",
        "    # Layer 1\n",
        "    Z1 = Xb @ W1 + b1\n",
        "    A1 = relu(Z1)\n",
        "    if using_dropout and p_drop1 > 0.0:\n",
        "        D1 = (np.random.rand(*A1.shape) > p_drop1).astype(A1.dtype)  # Bernoulli(1-p)\n",
        "        A1 = (A1 * D1) / (1.0 - p_drop1)\n",
        "    else:\n",
        "        D1 = None\n",
        "\n",
        "    # Layer 2\n",
        "    Z2 = A1 @ W2 + b2\n",
        "    A2 = relu(Z2)\n",
        "    if using_dropout and p_drop2 > 0.0:\n",
        "        D2 = (np.random.rand(*A2.shape) > p_drop2).astype(A2.dtype)\n",
        "        A2 = (A2 * D2) / (1.0 - p_drop2)\n",
        "    else:\n",
        "        D2 = None\n",
        "\n",
        "    # Output (linear)\n",
        "    Z3 = A2 @ W3 + b3\n",
        "    return Z1, A1, D1, Z2, A2, D2, Z3  # Z3 = y_hat (scaled)"
      ],
      "metadata": {
        "id": "dcdbrpxtxlBT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loss and Metrics"
      ],
      "metadata": {
        "id": "MrUefdTCxnlE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mse(y_true, y_pred):\n",
        "    return np.mean((y_pred - y_true) ** 2)\n",
        "\n",
        "def rmse(y_true, y_pred):\n",
        "    return np.sqrt(mse(y_true, y_pred))\n",
        "\n",
        "def mae(y_true, y_pred):\n",
        "    return np.mean(np.abs(y_pred - y_true))\n",
        "\n",
        "def r2_score(y_true, y_pred):\n",
        "    ss_res = np.sum((y_true - y_pred) ** 2)\n",
        "    ss_tot = np.sum((y_true - np.mean(y_true)) ** 2)\n",
        "    return 1.0 - (ss_res / (ss_tot + 1e-12))"
      ],
      "metadata": {
        "id": "JmEr28_3xqgq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Adam optimizer helpers"
      ],
      "metadata": {
        "id": "fhjj2xwpxr6I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Adam hyperparams\n",
        "lr = 1e-3\n",
        "beta1, beta2, eps = 0.9, 0.999, 1e-8\n",
        "\n",
        "# moments\n",
        "mW1 = np.zeros_like(W1); vW1 = np.zeros_like(W1)\n",
        "mW2 = np.zeros_like(W2); vW2 = np.zeros_like(W2)\n",
        "mW3 = np.zeros_like(W3); vW3 = np.zeros_like(W3)\n",
        "mb1 = np.zeros_like(b1); vb1 = np.zeros_like(b1)\n",
        "mb2 = np.zeros_like(b2); vb2 = np.zeros_like(b2)\n",
        "mb3 = np.zeros_like(b3); vb3 = np.zeros_like(b3)\n",
        "\n",
        "t_adam = 0  # timestep\n",
        "def adam_update(W, dW, mW, vW):\n",
        "    global t_adam\n",
        "    t_adam += 1\n",
        "    mW[:] = beta1 * mW + (1 - beta1) * dW\n",
        "    vW[:] = beta2 * vW + (1 - beta2) * (dW ** 2)\n",
        "    m_hat = mW / (1 - beta1 ** t_adam)\n",
        "    v_hat = vW / (1 - beta2 ** t_adam)\n",
        "    W -= lr * m_hat / (np.sqrt(v_hat) + eps)"
      ],
      "metadata": {
        "id": "LCqLPsTuxvXL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Backprop"
      ],
      "metadata": {
        "id": "2xiFlbuExw1e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "weight_decay = 1e-3  # đặt >0 (vd 1e-4) nếu muốn regularize\n",
        "\n",
        "def backward(Xb, yb, Z1, A1, D1, Z2, A2, D2, yhat):\n",
        "    n_b = Xb.shape[0]\n",
        "\n",
        "    # Output layer (MSE derivative)\n",
        "    dZ3 = (2.0 / n_b) * (yhat - yb)             # shape (B,1)\n",
        "    dW3 = A2.T @ dZ3 + weight_decay * W3        # (h2,1)\n",
        "    db3 = np.sum(dZ3, axis=0, keepdims=True)    # (1,1)\n",
        "\n",
        "    dA2 = dZ3 @ W3.T                             # (B,h2)\n",
        "    if D2 is not None:                           # dropout backward\n",
        "        dA2 = (dA2 * D2) / (1.0 - p_drop2)\n",
        "\n",
        "    dZ2 = dA2 * relu_grad(Z2)                    # (B,h2)\n",
        "    dW2 = A1.T @ dZ2 + weight_decay * W2         # (h1,h2)\n",
        "    db2 = np.sum(dZ2, axis=0, keepdims=True)     # (1,h2)\n",
        "\n",
        "    dA1 = dZ2 @ W2.T                             # (B,h1)\n",
        "    if D1 is not None:\n",
        "        dA1 = (dA1 * D1) / (1.0 - p_drop1)\n",
        "\n",
        "    dZ1 = dA1 * relu_grad(Z1)                    # (B,h1)\n",
        "    dW1 = Xb.T @ dZ1 + weight_decay * W1         # (d,h1)\n",
        "    db1 = np.sum(dZ1, axis=0, keepdims=True)     # (1,h1)\n",
        "\n",
        "    return dW1, db1, dW2, db2, dW3, db3"
      ],
      "metadata": {
        "id": "baJjei4hx13J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mini-batch + Early stopping"
      ],
      "metadata": {
        "id": "h4FJ6owux3vi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 1000\n",
        "patience = 20\n",
        "best_val = np.inf\n",
        "best_params = None\n",
        "pat = 0\n",
        "\n",
        "def get_minibatches(X, y, batch_size, seed=None):\n",
        "    rng = np.random.default_rng(seed)\n",
        "    idx = rng.permutation(X.shape[0])\n",
        "    for i in range(0, len(idx), batch_size):\n",
        "        sel = idx[i:i+batch_size]\n",
        "        yield X[sel], y[sel]\n",
        "\n",
        "for epoch in range(1, epochs+1):\n",
        "    # ----- Train -----\n",
        "    train_losses = []\n",
        "    for Xb, yb in get_minibatches(X_train_s, y_train_s, BATCH_SIZE, seed=epoch+SEED):\n",
        "        Z1, A1, D1, Z2, A2, D2, yhat = forward(Xb, USING_DROPOUT)\n",
        "        loss = mse(yb, yhat)\n",
        "        train_losses.append(loss)\n",
        "\n",
        "        dW1, db1, dW2, db2, dW3, db3 = backward(Xb, yb, Z1, A1, D1, Z2, A2, D2, yhat)\n",
        "\n",
        "        # Adam updates\n",
        "        adam_update(W1, dW1, mW1, vW1); adam_update(b1, db1, mb1, vb1)\n",
        "        adam_update(W2, dW2, mW2, vW2); adam_update(b2, db2, mb2, vb2)\n",
        "        adam_update(W3, dW3, mW3, vW3); adam_update(b3, db3, mb3, vb3)\n",
        "\n",
        "    # ----- Validation -----\n",
        "    _, _, _, _, _, _, yhat_val_s = forward(X_val_s)\n",
        "    val_rmse_scaled = rmse(y_val_s, yhat_val_s)\n",
        "\n",
        "    # In thêm metric ở thang đo gốc (dễ hiểu hơn)\n",
        "    yhat_val_orig = scaler_y.inverse_transform(yhat_val_s)\n",
        "    val_rmse = rmse(y_val, yhat_val_orig)\n",
        "    val_mae  = mae(y_val, yhat_val_orig)\n",
        "    val_r2   = r2_score(y_val, yhat_val_orig)\n",
        "\n",
        "    print(f\"Epoch {epoch:3d} | Train MSE(scaled)={np.mean(train_losses):.6f} | \"\n",
        "          f\"Val RMSE={val_rmse:.4f} | MAE={val_mae:.4f} | R2={val_r2:.4f}\")\n",
        "\n",
        "    # Early stopping theo RMSE (scaled để nhất quán với loss)\n",
        "    if val_rmse_scaled + 1e-8 < best_val:\n",
        "        best_val = val_rmse_scaled\n",
        "        best_params = (W1.copy(), b1.copy(), W2.copy(), b2.copy(), W3.copy(), b3.copy())\n",
        "        pat = 0\n",
        "    else:\n",
        "        pat += 1\n",
        "        if pat >= patience:\n",
        "            print(f\"Early stopping at epoch {epoch}. Best scaled RMSE={best_val:.6f}\")\n",
        "            break\n",
        "\n",
        "# Khôi phục tham số tốt nhất (nếu early stop)\n",
        "if best_params is not None:\n",
        "    W1, b1, W2, b2, W3, b3 = best_params"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hfFh70iwx6de",
        "outputId": "5e060d5c-ac00-4c56-ac49-3fc2c3c799d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch   1 | Train MSE(scaled)=4.022803 | Val RMSE=139553.8972 | MAE=111101.1563 | R2=-0.2073\n",
            "Epoch   2 | Train MSE(scaled)=1.190200 | Val RMSE=124805.7470 | MAE=98256.1235 | R2=0.0344\n",
            "Epoch   3 | Train MSE(scaled)=0.544028 | Val RMSE=82236.2928 | MAE=66215.8162 | R2=0.5808\n",
            "Epoch   4 | Train MSE(scaled)=0.297201 | Val RMSE=66147.1980 | MAE=52957.9149 | R2=0.7288\n",
            "Epoch   5 | Train MSE(scaled)=0.194304 | Val RMSE=58701.5907 | MAE=47010.3569 | R2=0.7864\n",
            "Epoch   6 | Train MSE(scaled)=0.144642 | Val RMSE=52204.8380 | MAE=42472.4714 | R2=0.8311\n",
            "Epoch   7 | Train MSE(scaled)=0.122279 | Val RMSE=48541.3128 | MAE=39358.4402 | R2=0.8539\n",
            "Epoch   8 | Train MSE(scaled)=0.105540 | Val RMSE=45471.8993 | MAE=37019.6612 | R2=0.8718\n",
            "Epoch   9 | Train MSE(scaled)=0.092314 | Val RMSE=42783.0285 | MAE=34974.7808 | R2=0.8865\n",
            "Epoch  10 | Train MSE(scaled)=0.082246 | Val RMSE=41344.9299 | MAE=33556.6736 | R2=0.8940\n",
            "Epoch  11 | Train MSE(scaled)=0.072821 | Val RMSE=39078.2897 | MAE=31800.3080 | R2=0.9053\n",
            "Epoch  12 | Train MSE(scaled)=0.064920 | Val RMSE=37707.2424 | MAE=30784.2512 | R2=0.9119\n",
            "Epoch  13 | Train MSE(scaled)=0.059641 | Val RMSE=36360.5252 | MAE=29784.9918 | R2=0.9180\n",
            "Epoch  14 | Train MSE(scaled)=0.056666 | Val RMSE=35288.6481 | MAE=28803.8397 | R2=0.9228\n",
            "Epoch  15 | Train MSE(scaled)=0.052092 | Val RMSE=34316.7571 | MAE=28108.4440 | R2=0.9270\n",
            "Epoch  16 | Train MSE(scaled)=0.047798 | Val RMSE=33305.9327 | MAE=27258.5974 | R2=0.9312\n",
            "Epoch  17 | Train MSE(scaled)=0.044750 | Val RMSE=32493.1530 | MAE=26445.2764 | R2=0.9345\n",
            "Epoch  18 | Train MSE(scaled)=0.041843 | Val RMSE=31573.4990 | MAE=25621.2042 | R2=0.9382\n",
            "Epoch  19 | Train MSE(scaled)=0.039779 | Val RMSE=30904.1733 | MAE=25032.1503 | R2=0.9408\n",
            "Epoch  20 | Train MSE(scaled)=0.037532 | Val RMSE=30064.5645 | MAE=24276.7506 | R2=0.9440\n",
            "Epoch  21 | Train MSE(scaled)=0.035648 | Val RMSE=29403.6849 | MAE=23803.3235 | R2=0.9464\n",
            "Epoch  22 | Train MSE(scaled)=0.034311 | Val RMSE=29145.7784 | MAE=23275.4377 | R2=0.9473\n",
            "Epoch  23 | Train MSE(scaled)=0.032980 | Val RMSE=28481.7631 | MAE=22985.3422 | R2=0.9497\n",
            "Epoch  24 | Train MSE(scaled)=0.031335 | Val RMSE=28135.1259 | MAE=22383.2062 | R2=0.9509\n",
            "Epoch  25 | Train MSE(scaled)=0.029655 | Val RMSE=27386.2852 | MAE=22112.2984 | R2=0.9535\n",
            "Epoch  26 | Train MSE(scaled)=0.028373 | Val RMSE=27146.9692 | MAE=21616.7856 | R2=0.9543\n",
            "Epoch  27 | Train MSE(scaled)=0.026862 | Val RMSE=26686.0371 | MAE=21167.2314 | R2=0.9559\n",
            "Epoch  28 | Train MSE(scaled)=0.026058 | Val RMSE=26233.8746 | MAE=20999.0206 | R2=0.9573\n",
            "Epoch  29 | Train MSE(scaled)=0.024888 | Val RMSE=25898.9472 | MAE=20628.9371 | R2=0.9584\n",
            "Epoch  30 | Train MSE(scaled)=0.023895 | Val RMSE=25706.6736 | MAE=20408.7215 | R2=0.9590\n",
            "Epoch  31 | Train MSE(scaled)=0.023115 | Val RMSE=25433.7388 | MAE=20168.0014 | R2=0.9599\n",
            "Epoch  32 | Train MSE(scaled)=0.022036 | Val RMSE=24987.5545 | MAE=19905.4198 | R2=0.9613\n",
            "Epoch  33 | Train MSE(scaled)=0.021214 | Val RMSE=24753.6143 | MAE=19577.3360 | R2=0.9620\n",
            "Epoch  34 | Train MSE(scaled)=0.020711 | Val RMSE=24347.5218 | MAE=19340.0891 | R2=0.9633\n",
            "Epoch  35 | Train MSE(scaled)=0.019775 | Val RMSE=24349.8274 | MAE=19207.1941 | R2=0.9632\n",
            "Epoch  36 | Train MSE(scaled)=0.019296 | Val RMSE=24035.9721 | MAE=19129.3752 | R2=0.9642\n",
            "Epoch  37 | Train MSE(scaled)=0.018318 | Val RMSE=23822.1475 | MAE=18856.5701 | R2=0.9648\n",
            "Epoch  38 | Train MSE(scaled)=0.017755 | Val RMSE=23724.9942 | MAE=18766.3613 | R2=0.9651\n",
            "Epoch  39 | Train MSE(scaled)=0.017135 | Val RMSE=23367.2883 | MAE=18615.9357 | R2=0.9662\n",
            "Epoch  40 | Train MSE(scaled)=0.016575 | Val RMSE=23246.9307 | MAE=18518.3589 | R2=0.9665\n",
            "Epoch  41 | Train MSE(scaled)=0.016078 | Val RMSE=23196.0877 | MAE=18523.9098 | R2=0.9666\n",
            "Epoch  42 | Train MSE(scaled)=0.015825 | Val RMSE=23135.2005 | MAE=18426.5912 | R2=0.9668\n",
            "Epoch  43 | Train MSE(scaled)=0.015558 | Val RMSE=22926.8871 | MAE=18372.3897 | R2=0.9674\n",
            "Epoch  44 | Train MSE(scaled)=0.014863 | Val RMSE=22485.6715 | MAE=17982.0350 | R2=0.9687\n",
            "Epoch  45 | Train MSE(scaled)=0.014711 | Val RMSE=22533.9389 | MAE=18038.1190 | R2=0.9685\n",
            "Epoch  46 | Train MSE(scaled)=0.014623 | Val RMSE=22781.2829 | MAE=17949.0737 | R2=0.9678\n",
            "Epoch  47 | Train MSE(scaled)=0.014096 | Val RMSE=22413.7641 | MAE=18029.3691 | R2=0.9689\n",
            "Epoch  48 | Train MSE(scaled)=0.013279 | Val RMSE=22299.8792 | MAE=17769.5958 | R2=0.9692\n",
            "Epoch  49 | Train MSE(scaled)=0.013253 | Val RMSE=22161.6664 | MAE=17756.1668 | R2=0.9696\n",
            "Epoch  50 | Train MSE(scaled)=0.013071 | Val RMSE=21898.9697 | MAE=17563.3738 | R2=0.9703\n",
            "Epoch  51 | Train MSE(scaled)=0.012339 | Val RMSE=21822.6801 | MAE=17383.1104 | R2=0.9705\n",
            "Epoch  52 | Train MSE(scaled)=0.011996 | Val RMSE=21763.8523 | MAE=17298.3951 | R2=0.9706\n",
            "Epoch  53 | Train MSE(scaled)=0.011878 | Val RMSE=21583.4748 | MAE=17393.7493 | R2=0.9711\n",
            "Epoch  54 | Train MSE(scaled)=0.011433 | Val RMSE=21525.0998 | MAE=17106.7164 | R2=0.9713\n",
            "Epoch  55 | Train MSE(scaled)=0.011210 | Val RMSE=21324.9078 | MAE=17137.2795 | R2=0.9718\n",
            "Epoch  56 | Train MSE(scaled)=0.011261 | Val RMSE=21614.4771 | MAE=17035.0470 | R2=0.9710\n",
            "Epoch  57 | Train MSE(scaled)=0.010743 | Val RMSE=20981.8872 | MAE=16823.6892 | R2=0.9727\n",
            "Epoch  58 | Train MSE(scaled)=0.010621 | Val RMSE=20991.1158 | MAE=16741.9448 | R2=0.9727\n",
            "Epoch  59 | Train MSE(scaled)=0.010132 | Val RMSE=20836.5012 | MAE=16696.3005 | R2=0.9731\n",
            "Epoch  60 | Train MSE(scaled)=0.010133 | Val RMSE=20903.0592 | MAE=16691.2721 | R2=0.9729\n",
            "Epoch  61 | Train MSE(scaled)=0.009645 | Val RMSE=20776.2411 | MAE=16562.2367 | R2=0.9732\n",
            "Epoch  62 | Train MSE(scaled)=0.009722 | Val RMSE=20449.5657 | MAE=16448.3153 | R2=0.9741\n",
            "Epoch  63 | Train MSE(scaled)=0.009486 | Val RMSE=20473.7663 | MAE=16346.0478 | R2=0.9740\n",
            "Epoch  64 | Train MSE(scaled)=0.009093 | Val RMSE=20575.3329 | MAE=16446.6221 | R2=0.9738\n",
            "Epoch  65 | Train MSE(scaled)=0.009004 | Val RMSE=20326.1131 | MAE=16271.4066 | R2=0.9744\n",
            "Epoch  66 | Train MSE(scaled)=0.008781 | Val RMSE=20425.4204 | MAE=16324.2421 | R2=0.9741\n",
            "Epoch  67 | Train MSE(scaled)=0.008555 | Val RMSE=20233.8494 | MAE=16190.1896 | R2=0.9746\n",
            "Epoch  68 | Train MSE(scaled)=0.008475 | Val RMSE=20135.9150 | MAE=16161.3767 | R2=0.9749\n",
            "Epoch  69 | Train MSE(scaled)=0.008207 | Val RMSE=20083.5162 | MAE=16095.4739 | R2=0.9750\n",
            "Epoch  70 | Train MSE(scaled)=0.008190 | Val RMSE=19949.7794 | MAE=16046.8115 | R2=0.9753\n",
            "Epoch  71 | Train MSE(scaled)=0.008004 | Val RMSE=20011.9190 | MAE=15915.8955 | R2=0.9752\n",
            "Epoch  72 | Train MSE(scaled)=0.007915 | Val RMSE=19851.2632 | MAE=15930.9076 | R2=0.9756\n",
            "Epoch  73 | Train MSE(scaled)=0.007733 | Val RMSE=19587.1733 | MAE=15718.4908 | R2=0.9762\n",
            "Epoch  74 | Train MSE(scaled)=0.007562 | Val RMSE=19587.5403 | MAE=15682.4645 | R2=0.9762\n",
            "Epoch  75 | Train MSE(scaled)=0.007524 | Val RMSE=19808.0784 | MAE=15718.2651 | R2=0.9757\n",
            "Epoch  76 | Train MSE(scaled)=0.007275 | Val RMSE=19445.9535 | MAE=15599.5168 | R2=0.9766\n",
            "Epoch  77 | Train MSE(scaled)=0.007008 | Val RMSE=19495.8290 | MAE=15537.6297 | R2=0.9764\n",
            "Epoch  78 | Train MSE(scaled)=0.006937 | Val RMSE=19307.6491 | MAE=15528.5412 | R2=0.9769\n",
            "Epoch  79 | Train MSE(scaled)=0.006816 | Val RMSE=19199.7179 | MAE=15376.5618 | R2=0.9771\n",
            "Epoch  80 | Train MSE(scaled)=0.006743 | Val RMSE=19151.5988 | MAE=15359.4620 | R2=0.9773\n",
            "Epoch  81 | Train MSE(scaled)=0.006668 | Val RMSE=19065.1250 | MAE=15322.7057 | R2=0.9775\n",
            "Epoch  82 | Train MSE(scaled)=0.006655 | Val RMSE=18938.6574 | MAE=15201.1024 | R2=0.9778\n",
            "Epoch  83 | Train MSE(scaled)=0.006285 | Val RMSE=19198.1319 | MAE=15328.7350 | R2=0.9772\n",
            "Epoch  84 | Train MSE(scaled)=0.006174 | Val RMSE=18754.3502 | MAE=15089.3352 | R2=0.9782\n",
            "Epoch  85 | Train MSE(scaled)=0.006213 | Val RMSE=18722.8617 | MAE=15013.1210 | R2=0.9783\n",
            "Epoch  86 | Train MSE(scaled)=0.006078 | Val RMSE=18852.9110 | MAE=15125.3092 | R2=0.9780\n",
            "Epoch  87 | Train MSE(scaled)=0.006025 | Val RMSE=18492.6637 | MAE=14986.4271 | R2=0.9788\n",
            "Epoch  88 | Train MSE(scaled)=0.005983 | Val RMSE=18627.5797 | MAE=14864.3438 | R2=0.9785\n",
            "Epoch  89 | Train MSE(scaled)=0.005657 | Val RMSE=18375.2949 | MAE=14851.5826 | R2=0.9791\n",
            "Epoch  90 | Train MSE(scaled)=0.005781 | Val RMSE=18621.8443 | MAE=14797.9624 | R2=0.9785\n",
            "Epoch  91 | Train MSE(scaled)=0.005550 | Val RMSE=18125.6923 | MAE=14718.3908 | R2=0.9796\n",
            "Epoch  92 | Train MSE(scaled)=0.005454 | Val RMSE=18082.9800 | MAE=14558.3216 | R2=0.9797\n",
            "Epoch  93 | Train MSE(scaled)=0.005287 | Val RMSE=17927.4826 | MAE=14426.6481 | R2=0.9801\n",
            "Epoch  94 | Train MSE(scaled)=0.005166 | Val RMSE=18205.2292 | MAE=14551.0974 | R2=0.9795\n",
            "Epoch  95 | Train MSE(scaled)=0.005237 | Val RMSE=17891.2724 | MAE=14479.9144 | R2=0.9802\n",
            "Epoch  96 | Train MSE(scaled)=0.005167 | Val RMSE=17952.0081 | MAE=14375.1231 | R2=0.9800\n",
            "Epoch  97 | Train MSE(scaled)=0.005019 | Val RMSE=17859.3141 | MAE=14385.5402 | R2=0.9802\n",
            "Epoch  98 | Train MSE(scaled)=0.005051 | Val RMSE=17738.8412 | MAE=14334.6990 | R2=0.9805\n",
            "Epoch  99 | Train MSE(scaled)=0.004853 | Val RMSE=17693.8718 | MAE=14332.6804 | R2=0.9806\n",
            "Epoch 100 | Train MSE(scaled)=0.004691 | Val RMSE=17676.0468 | MAE=14189.4272 | R2=0.9806\n",
            "Epoch 101 | Train MSE(scaled)=0.004736 | Val RMSE=17458.7784 | MAE=14120.9177 | R2=0.9811\n",
            "Epoch 102 | Train MSE(scaled)=0.004611 | Val RMSE=17438.4026 | MAE=14198.6806 | R2=0.9811\n",
            "Epoch 103 | Train MSE(scaled)=0.004616 | Val RMSE=17440.6785 | MAE=14072.9733 | R2=0.9811\n",
            "Epoch 104 | Train MSE(scaled)=0.004474 | Val RMSE=17326.9387 | MAE=13942.5918 | R2=0.9814\n",
            "Epoch 105 | Train MSE(scaled)=0.004613 | Val RMSE=17312.6686 | MAE=14037.5150 | R2=0.9814\n",
            "Epoch 106 | Train MSE(scaled)=0.004348 | Val RMSE=17044.7356 | MAE=13887.8037 | R2=0.9820\n",
            "Epoch 107 | Train MSE(scaled)=0.004283 | Val RMSE=17211.1538 | MAE=13945.7897 | R2=0.9816\n",
            "Epoch 108 | Train MSE(scaled)=0.004217 | Val RMSE=17106.9227 | MAE=13859.6299 | R2=0.9819\n",
            "Epoch 109 | Train MSE(scaled)=0.004077 | Val RMSE=16998.1696 | MAE=13870.7159 | R2=0.9821\n",
            "Epoch 110 | Train MSE(scaled)=0.004043 | Val RMSE=16924.2654 | MAE=13781.9187 | R2=0.9822\n",
            "Epoch 111 | Train MSE(scaled)=0.003950 | Val RMSE=16870.2657 | MAE=13755.5175 | R2=0.9824\n",
            "Epoch 112 | Train MSE(scaled)=0.004075 | Val RMSE=16688.3423 | MAE=13611.9293 | R2=0.9827\n",
            "Epoch 113 | Train MSE(scaled)=0.003869 | Val RMSE=16861.7979 | MAE=13681.3130 | R2=0.9824\n",
            "Epoch 114 | Train MSE(scaled)=0.003873 | Val RMSE=16706.4520 | MAE=13658.2620 | R2=0.9827\n",
            "Epoch 115 | Train MSE(scaled)=0.003739 | Val RMSE=16595.9254 | MAE=13493.2856 | R2=0.9829\n",
            "Epoch 116 | Train MSE(scaled)=0.003666 | Val RMSE=16516.6523 | MAE=13470.4103 | R2=0.9831\n",
            "Epoch 117 | Train MSE(scaled)=0.003732 | Val RMSE=16441.0782 | MAE=13498.6790 | R2=0.9832\n",
            "Epoch 118 | Train MSE(scaled)=0.003620 | Val RMSE=16595.3412 | MAE=13453.7043 | R2=0.9829\n",
            "Epoch 119 | Train MSE(scaled)=0.003642 | Val RMSE=16329.0554 | MAE=13380.1178 | R2=0.9835\n",
            "Epoch 120 | Train MSE(scaled)=0.003488 | Val RMSE=16337.2498 | MAE=13336.8253 | R2=0.9835\n",
            "Epoch 121 | Train MSE(scaled)=0.003460 | Val RMSE=16324.5250 | MAE=13338.0760 | R2=0.9835\n",
            "Epoch 122 | Train MSE(scaled)=0.003422 | Val RMSE=16171.4813 | MAE=13301.2560 | R2=0.9838\n",
            "Epoch 123 | Train MSE(scaled)=0.003458 | Val RMSE=16280.2308 | MAE=13283.7641 | R2=0.9836\n",
            "Epoch 124 | Train MSE(scaled)=0.003315 | Val RMSE=16141.9159 | MAE=13233.8929 | R2=0.9838\n",
            "Epoch 125 | Train MSE(scaled)=0.003254 | Val RMSE=16168.4971 | MAE=13261.5981 | R2=0.9838\n",
            "Epoch 126 | Train MSE(scaled)=0.003256 | Val RMSE=16040.8752 | MAE=13147.8793 | R2=0.9840\n",
            "Epoch 127 | Train MSE(scaled)=0.003219 | Val RMSE=16004.9082 | MAE=13125.6838 | R2=0.9841\n",
            "Epoch 128 | Train MSE(scaled)=0.003161 | Val RMSE=15845.5520 | MAE=13062.1330 | R2=0.9844\n",
            "Epoch 129 | Train MSE(scaled)=0.003114 | Val RMSE=15812.3485 | MAE=13031.7044 | R2=0.9845\n",
            "Epoch 130 | Train MSE(scaled)=0.003046 | Val RMSE=15797.0385 | MAE=12973.6335 | R2=0.9845\n",
            "Epoch 131 | Train MSE(scaled)=0.003060 | Val RMSE=15697.3997 | MAE=12906.9338 | R2=0.9847\n",
            "Epoch 132 | Train MSE(scaled)=0.002984 | Val RMSE=15598.1686 | MAE=12888.6506 | R2=0.9849\n",
            "Epoch 133 | Train MSE(scaled)=0.002928 | Val RMSE=15864.7117 | MAE=12948.4544 | R2=0.9844\n",
            "Epoch 134 | Train MSE(scaled)=0.003039 | Val RMSE=15546.8783 | MAE=12863.1899 | R2=0.9850\n",
            "Epoch 135 | Train MSE(scaled)=0.002880 | Val RMSE=15504.8169 | MAE=12794.5800 | R2=0.9851\n",
            "Epoch 136 | Train MSE(scaled)=0.002844 | Val RMSE=15620.0344 | MAE=12757.0033 | R2=0.9849\n",
            "Epoch 137 | Train MSE(scaled)=0.002786 | Val RMSE=15376.4454 | MAE=12694.0648 | R2=0.9853\n",
            "Epoch 138 | Train MSE(scaled)=0.002779 | Val RMSE=15472.9585 | MAE=12630.4075 | R2=0.9852\n",
            "Epoch 139 | Train MSE(scaled)=0.002681 | Val RMSE=15350.6060 | MAE=12606.8739 | R2=0.9854\n",
            "Epoch 140 | Train MSE(scaled)=0.002704 | Val RMSE=15261.9736 | MAE=12554.1318 | R2=0.9856\n",
            "Epoch 141 | Train MSE(scaled)=0.002633 | Val RMSE=15314.8173 | MAE=12545.1072 | R2=0.9855\n",
            "Epoch 142 | Train MSE(scaled)=0.002626 | Val RMSE=15198.8896 | MAE=12457.7475 | R2=0.9857\n",
            "Epoch 143 | Train MSE(scaled)=0.002631 | Val RMSE=15314.2305 | MAE=12508.4321 | R2=0.9855\n",
            "Epoch 144 | Train MSE(scaled)=0.002594 | Val RMSE=15022.1125 | MAE=12364.6136 | R2=0.9860\n",
            "Epoch 145 | Train MSE(scaled)=0.002539 | Val RMSE=15078.1889 | MAE=12402.8923 | R2=0.9859\n",
            "Epoch 146 | Train MSE(scaled)=0.002540 | Val RMSE=14946.7451 | MAE=12302.0922 | R2=0.9862\n",
            "Epoch 147 | Train MSE(scaled)=0.002473 | Val RMSE=14976.1860 | MAE=12225.4011 | R2=0.9861\n",
            "Epoch 148 | Train MSE(scaled)=0.002439 | Val RMSE=14868.1188 | MAE=12271.3825 | R2=0.9863\n",
            "Epoch 149 | Train MSE(scaled)=0.002455 | Val RMSE=14766.2729 | MAE=12147.7251 | R2=0.9865\n",
            "Epoch 150 | Train MSE(scaled)=0.002380 | Val RMSE=14810.8817 | MAE=12153.2595 | R2=0.9864\n",
            "Epoch 151 | Train MSE(scaled)=0.002338 | Val RMSE=14728.5670 | MAE=12064.6417 | R2=0.9866\n",
            "Epoch 152 | Train MSE(scaled)=0.002352 | Val RMSE=14726.2381 | MAE=12073.4749 | R2=0.9866\n",
            "Epoch 153 | Train MSE(scaled)=0.002307 | Val RMSE=14528.8635 | MAE=11966.8565 | R2=0.9869\n",
            "Epoch 154 | Train MSE(scaled)=0.002288 | Val RMSE=14568.7389 | MAE=12031.2316 | R2=0.9868\n",
            "Epoch 155 | Train MSE(scaled)=0.002290 | Val RMSE=14373.1010 | MAE=11896.2801 | R2=0.9872\n",
            "Epoch 156 | Train MSE(scaled)=0.002238 | Val RMSE=14508.4002 | MAE=11921.9157 | R2=0.9870\n",
            "Epoch 157 | Train MSE(scaled)=0.002180 | Val RMSE=14394.8424 | MAE=11843.8279 | R2=0.9872\n",
            "Epoch 158 | Train MSE(scaled)=0.002162 | Val RMSE=14390.1637 | MAE=11800.9537 | R2=0.9872\n",
            "Epoch 159 | Train MSE(scaled)=0.002168 | Val RMSE=14268.4295 | MAE=11713.7393 | R2=0.9874\n",
            "Epoch 160 | Train MSE(scaled)=0.002119 | Val RMSE=14187.8214 | MAE=11666.6112 | R2=0.9875\n",
            "Epoch 161 | Train MSE(scaled)=0.002087 | Val RMSE=14162.0317 | MAE=11669.4147 | R2=0.9876\n",
            "Epoch 162 | Train MSE(scaled)=0.002033 | Val RMSE=14149.9095 | MAE=11623.4303 | R2=0.9876\n",
            "Epoch 163 | Train MSE(scaled)=0.002043 | Val RMSE=14014.0255 | MAE=11538.1031 | R2=0.9878\n",
            "Epoch 164 | Train MSE(scaled)=0.002074 | Val RMSE=13915.3679 | MAE=11454.8195 | R2=0.9880\n",
            "Epoch 165 | Train MSE(scaled)=0.002031 | Val RMSE=13928.1537 | MAE=11487.0816 | R2=0.9880\n",
            "Epoch 166 | Train MSE(scaled)=0.001978 | Val RMSE=13940.9099 | MAE=11463.1186 | R2=0.9880\n",
            "Epoch 167 | Train MSE(scaled)=0.001938 | Val RMSE=13768.8445 | MAE=11364.7643 | R2=0.9882\n",
            "Epoch 168 | Train MSE(scaled)=0.001983 | Val RMSE=13825.1417 | MAE=11342.2585 | R2=0.9882\n",
            "Epoch 169 | Train MSE(scaled)=0.001981 | Val RMSE=13893.1968 | MAE=11376.6076 | R2=0.9880\n",
            "Epoch 170 | Train MSE(scaled)=0.001859 | Val RMSE=13624.1211 | MAE=11236.0182 | R2=0.9885\n",
            "Epoch 171 | Train MSE(scaled)=0.001889 | Val RMSE=13778.8001 | MAE=11256.3330 | R2=0.9882\n",
            "Epoch 172 | Train MSE(scaled)=0.001896 | Val RMSE=13638.9819 | MAE=11198.4763 | R2=0.9885\n",
            "Epoch 173 | Train MSE(scaled)=0.001936 | Val RMSE=13560.0458 | MAE=11185.2273 | R2=0.9886\n",
            "Epoch 174 | Train MSE(scaled)=0.001813 | Val RMSE=13568.9889 | MAE=11134.6673 | R2=0.9886\n",
            "Epoch 175 | Train MSE(scaled)=0.001788 | Val RMSE=13439.0885 | MAE=11097.0783 | R2=0.9888\n",
            "Epoch 176 | Train MSE(scaled)=0.001800 | Val RMSE=13722.6089 | MAE=11237.6574 | R2=0.9883\n",
            "Epoch 177 | Train MSE(scaled)=0.001843 | Val RMSE=13477.4344 | MAE=11054.8040 | R2=0.9887\n",
            "Epoch 178 | Train MSE(scaled)=0.001740 | Val RMSE=13447.8357 | MAE=11073.5368 | R2=0.9888\n",
            "Epoch 179 | Train MSE(scaled)=0.001788 | Val RMSE=13235.9288 | MAE=10991.8648 | R2=0.9891\n",
            "Epoch 180 | Train MSE(scaled)=0.001767 | Val RMSE=13318.5141 | MAE=10949.5910 | R2=0.9890\n",
            "Epoch 181 | Train MSE(scaled)=0.001711 | Val RMSE=13174.7268 | MAE=10835.8449 | R2=0.9892\n",
            "Epoch 182 | Train MSE(scaled)=0.001711 | Val RMSE=13225.6249 | MAE=10877.0576 | R2=0.9892\n",
            "Epoch 183 | Train MSE(scaled)=0.001675 | Val RMSE=13145.0694 | MAE=10797.5278 | R2=0.9893\n",
            "Epoch 184 | Train MSE(scaled)=0.001662 | Val RMSE=13042.9902 | MAE=10759.0368 | R2=0.9895\n",
            "Epoch 185 | Train MSE(scaled)=0.001609 | Val RMSE=13162.4534 | MAE=10810.7704 | R2=0.9893\n",
            "Epoch 186 | Train MSE(scaled)=0.001630 | Val RMSE=13024.7870 | MAE=10723.8107 | R2=0.9895\n",
            "Epoch 187 | Train MSE(scaled)=0.001603 | Val RMSE=12911.5423 | MAE=10670.2220 | R2=0.9897\n",
            "Epoch 188 | Train MSE(scaled)=0.001624 | Val RMSE=12909.9954 | MAE=10583.5848 | R2=0.9897\n",
            "Epoch 189 | Train MSE(scaled)=0.001582 | Val RMSE=12809.5607 | MAE=10570.7362 | R2=0.9898\n",
            "Epoch 190 | Train MSE(scaled)=0.001557 | Val RMSE=12778.8929 | MAE=10535.4607 | R2=0.9899\n",
            "Epoch 191 | Train MSE(scaled)=0.001570 | Val RMSE=12684.9181 | MAE=10470.3188 | R2=0.9900\n",
            "Epoch 192 | Train MSE(scaled)=0.001537 | Val RMSE=12698.1442 | MAE=10468.1419 | R2=0.9900\n",
            "Epoch 193 | Train MSE(scaled)=0.001570 | Val RMSE=12722.6740 | MAE=10441.2036 | R2=0.9900\n",
            "Epoch 194 | Train MSE(scaled)=0.001519 | Val RMSE=12720.3598 | MAE=10439.3774 | R2=0.9900\n",
            "Epoch 195 | Train MSE(scaled)=0.001495 | Val RMSE=12581.6428 | MAE=10387.0665 | R2=0.9902\n",
            "Epoch 196 | Train MSE(scaled)=0.001492 | Val RMSE=12538.3933 | MAE=10320.6267 | R2=0.9903\n",
            "Epoch 197 | Train MSE(scaled)=0.001509 | Val RMSE=12445.2167 | MAE=10237.0938 | R2=0.9904\n",
            "Epoch 198 | Train MSE(scaled)=0.001445 | Val RMSE=12493.8734 | MAE=10287.4064 | R2=0.9903\n",
            "Epoch 199 | Train MSE(scaled)=0.001398 | Val RMSE=12332.2528 | MAE=10166.4652 | R2=0.9906\n",
            "Epoch 200 | Train MSE(scaled)=0.001403 | Val RMSE=12332.0494 | MAE=10181.6466 | R2=0.9906\n",
            "Epoch 201 | Train MSE(scaled)=0.001424 | Val RMSE=12364.4754 | MAE=10170.8695 | R2=0.9905\n",
            "Epoch 202 | Train MSE(scaled)=0.001439 | Val RMSE=12195.2050 | MAE=10042.5477 | R2=0.9908\n",
            "Epoch 203 | Train MSE(scaled)=0.001385 | Val RMSE=12235.8291 | MAE=10083.7014 | R2=0.9907\n",
            "Epoch 204 | Train MSE(scaled)=0.001375 | Val RMSE=12112.3775 | MAE=10044.0726 | R2=0.9909\n",
            "Epoch 205 | Train MSE(scaled)=0.001355 | Val RMSE=12137.7899 | MAE=10037.0826 | R2=0.9909\n",
            "Epoch 206 | Train MSE(scaled)=0.001355 | Val RMSE=12148.6233 | MAE=9979.9892 | R2=0.9909\n",
            "Epoch 207 | Train MSE(scaled)=0.001341 | Val RMSE=11994.0588 | MAE=9855.9874 | R2=0.9911\n",
            "Epoch 208 | Train MSE(scaled)=0.001345 | Val RMSE=11969.3300 | MAE=9888.2555 | R2=0.9911\n",
            "Epoch 209 | Train MSE(scaled)=0.001347 | Val RMSE=11912.0261 | MAE=9920.9955 | R2=0.9912\n",
            "Epoch 210 | Train MSE(scaled)=0.001341 | Val RMSE=11812.2110 | MAE=9800.4291 | R2=0.9914\n",
            "Epoch 211 | Train MSE(scaled)=0.001319 | Val RMSE=11761.5389 | MAE=9727.3570 | R2=0.9914\n",
            "Epoch 212 | Train MSE(scaled)=0.001349 | Val RMSE=11714.4943 | MAE=9737.5521 | R2=0.9915\n",
            "Epoch 213 | Train MSE(scaled)=0.001343 | Val RMSE=11657.8400 | MAE=9627.8961 | R2=0.9916\n",
            "Epoch 214 | Train MSE(scaled)=0.001275 | Val RMSE=11677.2537 | MAE=9674.8536 | R2=0.9915\n",
            "Epoch 215 | Train MSE(scaled)=0.001273 | Val RMSE=11658.9531 | MAE=9634.4218 | R2=0.9916\n",
            "Epoch 216 | Train MSE(scaled)=0.001264 | Val RMSE=11515.7610 | MAE=9583.4961 | R2=0.9918\n",
            "Epoch 217 | Train MSE(scaled)=0.001259 | Val RMSE=11472.0402 | MAE=9475.8441 | R2=0.9918\n",
            "Epoch 218 | Train MSE(scaled)=0.001243 | Val RMSE=11427.6366 | MAE=9435.5516 | R2=0.9919\n",
            "Epoch 219 | Train MSE(scaled)=0.001225 | Val RMSE=11387.8572 | MAE=9445.9353 | R2=0.9920\n",
            "Epoch 220 | Train MSE(scaled)=0.001226 | Val RMSE=11245.9765 | MAE=9367.3864 | R2=0.9922\n",
            "Epoch 221 | Train MSE(scaled)=0.001235 | Val RMSE=11263.1642 | MAE=9309.8440 | R2=0.9921\n",
            "Epoch 222 | Train MSE(scaled)=0.001232 | Val RMSE=11281.3283 | MAE=9311.2028 | R2=0.9921\n",
            "Epoch 223 | Train MSE(scaled)=0.001208 | Val RMSE=11301.6456 | MAE=9281.4109 | R2=0.9921\n",
            "Epoch 224 | Train MSE(scaled)=0.001228 | Val RMSE=11103.2113 | MAE=9209.8710 | R2=0.9924\n",
            "Epoch 225 | Train MSE(scaled)=0.001177 | Val RMSE=11191.5204 | MAE=9182.7064 | R2=0.9922\n",
            "Epoch 226 | Train MSE(scaled)=0.001190 | Val RMSE=11218.0018 | MAE=9184.1976 | R2=0.9922\n",
            "Epoch 227 | Train MSE(scaled)=0.001226 | Val RMSE=10884.7178 | MAE=9031.7316 | R2=0.9927\n",
            "Epoch 228 | Train MSE(scaled)=0.001183 | Val RMSE=10793.0892 | MAE=8984.0301 | R2=0.9928\n",
            "Epoch 229 | Train MSE(scaled)=0.001142 | Val RMSE=10922.5850 | MAE=9063.4570 | R2=0.9926\n",
            "Epoch 230 | Train MSE(scaled)=0.001170 | Val RMSE=10768.8918 | MAE=8964.1853 | R2=0.9928\n",
            "Epoch 231 | Train MSE(scaled)=0.001159 | Val RMSE=10712.7639 | MAE=8951.9679 | R2=0.9929\n",
            "Epoch 232 | Train MSE(scaled)=0.001144 | Val RMSE=10614.3011 | MAE=8818.5107 | R2=0.9930\n",
            "Epoch 233 | Train MSE(scaled)=0.001136 | Val RMSE=10607.8463 | MAE=8849.3008 | R2=0.9930\n",
            "Epoch 234 | Train MSE(scaled)=0.001110 | Val RMSE=10579.9803 | MAE=8783.0041 | R2=0.9931\n",
            "Epoch 235 | Train MSE(scaled)=0.001124 | Val RMSE=10574.0770 | MAE=8704.7007 | R2=0.9931\n",
            "Epoch 236 | Train MSE(scaled)=0.001092 | Val RMSE=10521.9249 | MAE=8688.5673 | R2=0.9931\n",
            "Epoch 237 | Train MSE(scaled)=0.001075 | Val RMSE=10612.9077 | MAE=8724.9182 | R2=0.9930\n",
            "Epoch 238 | Train MSE(scaled)=0.001117 | Val RMSE=10624.8105 | MAE=8684.5181 | R2=0.9930\n",
            "Epoch 239 | Train MSE(scaled)=0.001141 | Val RMSE=10491.4554 | MAE=8617.5768 | R2=0.9932\n",
            "Epoch 240 | Train MSE(scaled)=0.001094 | Val RMSE=10331.1010 | MAE=8463.5225 | R2=0.9934\n",
            "Epoch 241 | Train MSE(scaled)=0.001055 | Val RMSE=10276.0711 | MAE=8544.9762 | R2=0.9935\n",
            "Epoch 242 | Train MSE(scaled)=0.001041 | Val RMSE=10341.7099 | MAE=8522.7236 | R2=0.9934\n",
            "Epoch 243 | Train MSE(scaled)=0.001103 | Val RMSE=10211.0816 | MAE=8395.6325 | R2=0.9935\n",
            "Epoch 244 | Train MSE(scaled)=0.001023 | Val RMSE=10107.3881 | MAE=8414.3726 | R2=0.9937\n",
            "Epoch 245 | Train MSE(scaled)=0.001026 | Val RMSE=10113.2760 | MAE=8422.6943 | R2=0.9937\n",
            "Epoch 246 | Train MSE(scaled)=0.001023 | Val RMSE=10191.5551 | MAE=8357.9812 | R2=0.9936\n",
            "Epoch 247 | Train MSE(scaled)=0.000988 | Val RMSE=10089.0988 | MAE=8310.7081 | R2=0.9937\n",
            "Epoch 248 | Train MSE(scaled)=0.001061 | Val RMSE=10045.8189 | MAE=8285.7773 | R2=0.9937\n",
            "Epoch 249 | Train MSE(scaled)=0.001024 | Val RMSE=9897.8034 | MAE=8159.1251 | R2=0.9939\n",
            "Epoch 250 | Train MSE(scaled)=0.000991 | Val RMSE=9853.4234 | MAE=8227.7878 | R2=0.9940\n",
            "Epoch 251 | Train MSE(scaled)=0.000993 | Val RMSE=9809.5630 | MAE=8133.2215 | R2=0.9940\n",
            "Epoch 252 | Train MSE(scaled)=0.000982 | Val RMSE=9846.7753 | MAE=8100.9340 | R2=0.9940\n",
            "Epoch 253 | Train MSE(scaled)=0.000994 | Val RMSE=9777.3437 | MAE=8086.0670 | R2=0.9941\n",
            "Epoch 254 | Train MSE(scaled)=0.000988 | Val RMSE=9831.6215 | MAE=8140.2645 | R2=0.9940\n",
            "Epoch 255 | Train MSE(scaled)=0.001009 | Val RMSE=9915.4889 | MAE=8088.2918 | R2=0.9939\n",
            "Epoch 256 | Train MSE(scaled)=0.000980 | Val RMSE=9740.2851 | MAE=8027.0036 | R2=0.9941\n",
            "Epoch 257 | Train MSE(scaled)=0.000948 | Val RMSE=9651.7915 | MAE=8037.9592 | R2=0.9942\n",
            "Epoch 258 | Train MSE(scaled)=0.000956 | Val RMSE=9467.6655 | MAE=7800.4893 | R2=0.9944\n",
            "Epoch 259 | Train MSE(scaled)=0.000915 | Val RMSE=9659.2196 | MAE=7967.9106 | R2=0.9942\n",
            "Epoch 260 | Train MSE(scaled)=0.000923 | Val RMSE=9580.4655 | MAE=7890.3901 | R2=0.9943\n",
            "Epoch 261 | Train MSE(scaled)=0.000902 | Val RMSE=9482.8153 | MAE=7793.2189 | R2=0.9944\n",
            "Epoch 262 | Train MSE(scaled)=0.000929 | Val RMSE=9394.9005 | MAE=7812.7834 | R2=0.9945\n",
            "Epoch 263 | Train MSE(scaled)=0.000920 | Val RMSE=9430.3842 | MAE=7843.2414 | R2=0.9945\n",
            "Epoch 264 | Train MSE(scaled)=0.000898 | Val RMSE=9326.9662 | MAE=7707.2720 | R2=0.9946\n",
            "Epoch 265 | Train MSE(scaled)=0.000902 | Val RMSE=9354.1250 | MAE=7795.3874 | R2=0.9946\n",
            "Epoch 266 | Train MSE(scaled)=0.000892 | Val RMSE=9250.1671 | MAE=7695.1405 | R2=0.9947\n",
            "Epoch 267 | Train MSE(scaled)=0.000897 | Val RMSE=9258.2531 | MAE=7664.9979 | R2=0.9947\n",
            "Epoch 268 | Train MSE(scaled)=0.000891 | Val RMSE=9279.2546 | MAE=7688.1899 | R2=0.9947\n",
            "Epoch 269 | Train MSE(scaled)=0.000871 | Val RMSE=9226.2273 | MAE=7636.1967 | R2=0.9947\n",
            "Epoch 270 | Train MSE(scaled)=0.000857 | Val RMSE=9145.2439 | MAE=7587.6940 | R2=0.9948\n",
            "Epoch 271 | Train MSE(scaled)=0.000862 | Val RMSE=9240.5456 | MAE=7559.7581 | R2=0.9947\n",
            "Epoch 272 | Train MSE(scaled)=0.000859 | Val RMSE=9263.9665 | MAE=7554.8236 | R2=0.9947\n",
            "Epoch 273 | Train MSE(scaled)=0.000834 | Val RMSE=9211.0525 | MAE=7531.5161 | R2=0.9947\n",
            "Epoch 274 | Train MSE(scaled)=0.000861 | Val RMSE=9319.3677 | MAE=7551.0273 | R2=0.9946\n",
            "Epoch 275 | Train MSE(scaled)=0.000863 | Val RMSE=9091.4506 | MAE=7424.3461 | R2=0.9949\n",
            "Epoch 276 | Train MSE(scaled)=0.000829 | Val RMSE=8954.2989 | MAE=7401.4428 | R2=0.9950\n",
            "Epoch 277 | Train MSE(scaled)=0.000822 | Val RMSE=8931.3160 | MAE=7347.2640 | R2=0.9951\n",
            "Epoch 278 | Train MSE(scaled)=0.000800 | Val RMSE=9028.6306 | MAE=7399.8315 | R2=0.9949\n",
            "Epoch 279 | Train MSE(scaled)=0.000799 | Val RMSE=8991.8774 | MAE=7395.9802 | R2=0.9950\n",
            "Epoch 280 | Train MSE(scaled)=0.000798 | Val RMSE=8865.1758 | MAE=7295.7782 | R2=0.9951\n",
            "Epoch 281 | Train MSE(scaled)=0.000792 | Val RMSE=8754.7514 | MAE=7271.4640 | R2=0.9952\n",
            "Epoch 282 | Train MSE(scaled)=0.000782 | Val RMSE=8866.1792 | MAE=7289.4472 | R2=0.9951\n",
            "Epoch 283 | Train MSE(scaled)=0.000770 | Val RMSE=8835.0905 | MAE=7268.7353 | R2=0.9952\n",
            "Epoch 284 | Train MSE(scaled)=0.000764 | Val RMSE=8727.4153 | MAE=7159.3709 | R2=0.9953\n",
            "Epoch 285 | Train MSE(scaled)=0.000802 | Val RMSE=8647.0847 | MAE=7162.7793 | R2=0.9954\n",
            "Epoch 286 | Train MSE(scaled)=0.000760 | Val RMSE=8645.6706 | MAE=7145.1040 | R2=0.9954\n",
            "Epoch 287 | Train MSE(scaled)=0.000780 | Val RMSE=8547.0730 | MAE=7069.1703 | R2=0.9955\n",
            "Epoch 288 | Train MSE(scaled)=0.000749 | Val RMSE=8579.6671 | MAE=7131.8074 | R2=0.9954\n",
            "Epoch 289 | Train MSE(scaled)=0.000762 | Val RMSE=8646.4508 | MAE=7159.7591 | R2=0.9954\n",
            "Epoch 290 | Train MSE(scaled)=0.000770 | Val RMSE=8449.5920 | MAE=6984.4248 | R2=0.9956\n",
            "Epoch 291 | Train MSE(scaled)=0.000748 | Val RMSE=8434.2406 | MAE=7013.4088 | R2=0.9956\n",
            "Epoch 292 | Train MSE(scaled)=0.000762 | Val RMSE=8472.8003 | MAE=6996.8947 | R2=0.9955\n",
            "Epoch 293 | Train MSE(scaled)=0.000746 | Val RMSE=8480.4204 | MAE=7060.3670 | R2=0.9955\n",
            "Epoch 294 | Train MSE(scaled)=0.000736 | Val RMSE=8346.2618 | MAE=6857.7107 | R2=0.9957\n",
            "Epoch 295 | Train MSE(scaled)=0.000727 | Val RMSE=8496.3502 | MAE=6976.4183 | R2=0.9955\n",
            "Epoch 296 | Train MSE(scaled)=0.000731 | Val RMSE=8436.4785 | MAE=6966.1684 | R2=0.9956\n",
            "Epoch 297 | Train MSE(scaled)=0.000719 | Val RMSE=8175.5504 | MAE=6696.7259 | R2=0.9959\n",
            "Epoch 298 | Train MSE(scaled)=0.000724 | Val RMSE=8469.9947 | MAE=6948.2953 | R2=0.9956\n",
            "Epoch 299 | Train MSE(scaled)=0.000716 | Val RMSE=8210.9614 | MAE=6741.9255 | R2=0.9958\n",
            "Epoch 300 | Train MSE(scaled)=0.000689 | Val RMSE=8281.5535 | MAE=6855.3652 | R2=0.9957\n",
            "Epoch 301 | Train MSE(scaled)=0.000724 | Val RMSE=8109.2482 | MAE=6727.7315 | R2=0.9959\n",
            "Epoch 302 | Train MSE(scaled)=0.000723 | Val RMSE=8026.3434 | MAE=6667.8496 | R2=0.9960\n",
            "Epoch 303 | Train MSE(scaled)=0.000712 | Val RMSE=8100.4571 | MAE=6718.8334 | R2=0.9959\n",
            "Epoch 304 | Train MSE(scaled)=0.000669 | Val RMSE=8185.9404 | MAE=6780.0042 | R2=0.9958\n",
            "Epoch 305 | Train MSE(scaled)=0.000666 | Val RMSE=7989.5709 | MAE=6616.6252 | R2=0.9960\n",
            "Epoch 306 | Train MSE(scaled)=0.000677 | Val RMSE=8037.9450 | MAE=6720.3749 | R2=0.9960\n",
            "Epoch 307 | Train MSE(scaled)=0.000674 | Val RMSE=8101.6843 | MAE=6724.4495 | R2=0.9959\n",
            "Epoch 308 | Train MSE(scaled)=0.000694 | Val RMSE=8041.2040 | MAE=6708.8942 | R2=0.9960\n",
            "Epoch 309 | Train MSE(scaled)=0.000690 | Val RMSE=8047.6973 | MAE=6688.6898 | R2=0.9960\n",
            "Epoch 310 | Train MSE(scaled)=0.000667 | Val RMSE=7884.6702 | MAE=6591.3060 | R2=0.9961\n",
            "Epoch 311 | Train MSE(scaled)=0.000658 | Val RMSE=7917.4890 | MAE=6596.0603 | R2=0.9961\n",
            "Epoch 312 | Train MSE(scaled)=0.000646 | Val RMSE=7999.9806 | MAE=6671.7175 | R2=0.9960\n",
            "Epoch 313 | Train MSE(scaled)=0.000670 | Val RMSE=7724.0977 | MAE=6416.7373 | R2=0.9963\n",
            "Epoch 314 | Train MSE(scaled)=0.000643 | Val RMSE=8122.1132 | MAE=6709.6914 | R2=0.9959\n",
            "Epoch 315 | Train MSE(scaled)=0.000659 | Val RMSE=7930.3336 | MAE=6578.6237 | R2=0.9961\n",
            "Epoch 316 | Train MSE(scaled)=0.000684 | Val RMSE=8178.3873 | MAE=6747.4763 | R2=0.9959\n",
            "Epoch 317 | Train MSE(scaled)=0.000685 | Val RMSE=8022.3111 | MAE=6688.0994 | R2=0.9960\n",
            "Epoch 318 | Train MSE(scaled)=0.000676 | Val RMSE=7843.5381 | MAE=6500.4348 | R2=0.9962\n",
            "Epoch 319 | Train MSE(scaled)=0.000674 | Val RMSE=7718.9537 | MAE=6415.9255 | R2=0.9963\n",
            "Epoch 320 | Train MSE(scaled)=0.000645 | Val RMSE=7967.5421 | MAE=6625.2340 | R2=0.9961\n",
            "Epoch 321 | Train MSE(scaled)=0.000626 | Val RMSE=7886.6713 | MAE=6544.2084 | R2=0.9961\n",
            "Epoch 322 | Train MSE(scaled)=0.000652 | Val RMSE=7536.4897 | MAE=6316.3553 | R2=0.9965\n",
            "Epoch 323 | Train MSE(scaled)=0.000631 | Val RMSE=7863.1855 | MAE=6500.0117 | R2=0.9962\n",
            "Epoch 324 | Train MSE(scaled)=0.000622 | Val RMSE=7745.9711 | MAE=6467.9810 | R2=0.9963\n",
            "Epoch 325 | Train MSE(scaled)=0.000608 | Val RMSE=7608.3108 | MAE=6347.4724 | R2=0.9964\n",
            "Epoch 326 | Train MSE(scaled)=0.000626 | Val RMSE=7626.5613 | MAE=6333.2815 | R2=0.9964\n",
            "Epoch 327 | Train MSE(scaled)=0.000629 | Val RMSE=7797.9835 | MAE=6415.3378 | R2=0.9962\n",
            "Epoch 328 | Train MSE(scaled)=0.000611 | Val RMSE=7613.0666 | MAE=6385.6639 | R2=0.9964\n",
            "Epoch 329 | Train MSE(scaled)=0.000616 | Val RMSE=7523.1718 | MAE=6243.4466 | R2=0.9965\n",
            "Epoch 330 | Train MSE(scaled)=0.000611 | Val RMSE=7291.8626 | MAE=6079.0027 | R2=0.9967\n",
            "Epoch 331 | Train MSE(scaled)=0.000606 | Val RMSE=7580.3696 | MAE=6314.8129 | R2=0.9964\n",
            "Epoch 332 | Train MSE(scaled)=0.000603 | Val RMSE=7354.3053 | MAE=6158.2550 | R2=0.9966\n",
            "Epoch 333 | Train MSE(scaled)=0.000589 | Val RMSE=7267.2013 | MAE=6042.0733 | R2=0.9967\n",
            "Epoch 334 | Train MSE(scaled)=0.000607 | Val RMSE=7271.7959 | MAE=6072.1652 | R2=0.9967\n",
            "Epoch 335 | Train MSE(scaled)=0.000570 | Val RMSE=7403.0890 | MAE=6179.7684 | R2=0.9966\n",
            "Epoch 336 | Train MSE(scaled)=0.000568 | Val RMSE=7328.8253 | MAE=6082.5354 | R2=0.9967\n",
            "Epoch 337 | Train MSE(scaled)=0.000578 | Val RMSE=7284.0918 | MAE=6021.0456 | R2=0.9967\n",
            "Epoch 338 | Train MSE(scaled)=0.000575 | Val RMSE=7283.8741 | MAE=6044.4520 | R2=0.9967\n",
            "Epoch 339 | Train MSE(scaled)=0.000579 | Val RMSE=7163.4583 | MAE=5974.2527 | R2=0.9968\n",
            "Epoch 340 | Train MSE(scaled)=0.000603 | Val RMSE=7132.7510 | MAE=5915.9644 | R2=0.9968\n",
            "Epoch 341 | Train MSE(scaled)=0.000597 | Val RMSE=7090.1409 | MAE=5853.8705 | R2=0.9969\n",
            "Epoch 342 | Train MSE(scaled)=0.000583 | Val RMSE=7178.1228 | MAE=5944.4451 | R2=0.9968\n",
            "Epoch 343 | Train MSE(scaled)=0.000566 | Val RMSE=7077.1129 | MAE=5904.2882 | R2=0.9969\n",
            "Epoch 344 | Train MSE(scaled)=0.000559 | Val RMSE=7191.3077 | MAE=5936.3531 | R2=0.9968\n",
            "Epoch 345 | Train MSE(scaled)=0.000563 | Val RMSE=6887.7661 | MAE=5729.2021 | R2=0.9971\n",
            "Epoch 346 | Train MSE(scaled)=0.000557 | Val RMSE=6956.6826 | MAE=5759.3261 | R2=0.9970\n",
            "Epoch 347 | Train MSE(scaled)=0.000577 | Val RMSE=6907.4826 | MAE=5691.7374 | R2=0.9970\n",
            "Epoch 348 | Train MSE(scaled)=0.000558 | Val RMSE=6896.2370 | MAE=5687.1015 | R2=0.9971\n",
            "Epoch 349 | Train MSE(scaled)=0.000558 | Val RMSE=7055.2445 | MAE=5828.4837 | R2=0.9969\n",
            "Epoch 350 | Train MSE(scaled)=0.000553 | Val RMSE=6640.8732 | MAE=5467.8716 | R2=0.9973\n",
            "Epoch 351 | Train MSE(scaled)=0.000541 | Val RMSE=6792.2171 | MAE=5660.1153 | R2=0.9971\n",
            "Epoch 352 | Train MSE(scaled)=0.000526 | Val RMSE=6782.8367 | MAE=5613.0047 | R2=0.9971\n",
            "Epoch 353 | Train MSE(scaled)=0.000540 | Val RMSE=6801.1317 | MAE=5597.9688 | R2=0.9971\n",
            "Epoch 354 | Train MSE(scaled)=0.000523 | Val RMSE=6951.3883 | MAE=5688.4085 | R2=0.9970\n",
            "Epoch 355 | Train MSE(scaled)=0.000547 | Val RMSE=6672.6979 | MAE=5498.2400 | R2=0.9972\n",
            "Epoch 356 | Train MSE(scaled)=0.000511 | Val RMSE=6661.8125 | MAE=5612.8503 | R2=0.9972\n",
            "Epoch 357 | Train MSE(scaled)=0.000524 | Val RMSE=6475.8804 | MAE=5365.3222 | R2=0.9974\n",
            "Epoch 358 | Train MSE(scaled)=0.000524 | Val RMSE=6589.7307 | MAE=5453.9517 | R2=0.9973\n",
            "Epoch 359 | Train MSE(scaled)=0.000507 | Val RMSE=6723.7048 | MAE=5601.1386 | R2=0.9972\n",
            "Epoch 360 | Train MSE(scaled)=0.000505 | Val RMSE=6567.9645 | MAE=5392.8927 | R2=0.9973\n",
            "Epoch 361 | Train MSE(scaled)=0.000524 | Val RMSE=6595.7426 | MAE=5512.8927 | R2=0.9973\n",
            "Epoch 362 | Train MSE(scaled)=0.000541 | Val RMSE=6295.7336 | MAE=5216.7019 | R2=0.9975\n",
            "Epoch 363 | Train MSE(scaled)=0.000513 | Val RMSE=6410.2571 | MAE=5308.0158 | R2=0.9975\n",
            "Epoch 364 | Train MSE(scaled)=0.000537 | Val RMSE=6582.3719 | MAE=5443.1354 | R2=0.9973\n",
            "Epoch 365 | Train MSE(scaled)=0.000508 | Val RMSE=6345.6400 | MAE=5226.0330 | R2=0.9975\n",
            "Epoch 366 | Train MSE(scaled)=0.000487 | Val RMSE=6397.7117 | MAE=5305.2733 | R2=0.9975\n",
            "Epoch 367 | Train MSE(scaled)=0.000496 | Val RMSE=6347.8688 | MAE=5224.8236 | R2=0.9975\n",
            "Epoch 368 | Train MSE(scaled)=0.000495 | Val RMSE=6642.8229 | MAE=5430.2598 | R2=0.9973\n",
            "Epoch 369 | Train MSE(scaled)=0.000513 | Val RMSE=6441.8733 | MAE=5240.2896 | R2=0.9974\n",
            "Epoch 370 | Train MSE(scaled)=0.000477 | Val RMSE=6109.1430 | MAE=5052.1012 | R2=0.9977\n",
            "Epoch 371 | Train MSE(scaled)=0.000479 | Val RMSE=6386.2962 | MAE=5270.4723 | R2=0.9975\n",
            "Epoch 372 | Train MSE(scaled)=0.000479 | Val RMSE=6144.8966 | MAE=5010.9172 | R2=0.9977\n",
            "Epoch 373 | Train MSE(scaled)=0.000470 | Val RMSE=6127.6527 | MAE=5061.5457 | R2=0.9977\n",
            "Epoch 374 | Train MSE(scaled)=0.000458 | Val RMSE=5945.9951 | MAE=4947.6309 | R2=0.9978\n",
            "Epoch 375 | Train MSE(scaled)=0.000476 | Val RMSE=5928.1509 | MAE=4921.9926 | R2=0.9978\n",
            "Epoch 376 | Train MSE(scaled)=0.000454 | Val RMSE=6025.8730 | MAE=4978.2650 | R2=0.9977\n",
            "Epoch 377 | Train MSE(scaled)=0.000455 | Val RMSE=5950.5318 | MAE=4888.0377 | R2=0.9978\n",
            "Epoch 378 | Train MSE(scaled)=0.000437 | Val RMSE=5793.0670 | MAE=4819.2128 | R2=0.9979\n",
            "Epoch 379 | Train MSE(scaled)=0.000443 | Val RMSE=5863.6748 | MAE=4852.1979 | R2=0.9979\n",
            "Epoch 380 | Train MSE(scaled)=0.000455 | Val RMSE=5854.5990 | MAE=4817.0905 | R2=0.9979\n",
            "Epoch 381 | Train MSE(scaled)=0.000478 | Val RMSE=5738.0770 | MAE=4623.7669 | R2=0.9980\n",
            "Epoch 382 | Train MSE(scaled)=0.000481 | Val RMSE=5871.4155 | MAE=4761.0252 | R2=0.9979\n",
            "Epoch 383 | Train MSE(scaled)=0.000457 | Val RMSE=5924.7144 | MAE=4816.1980 | R2=0.9978\n",
            "Epoch 384 | Train MSE(scaled)=0.000440 | Val RMSE=6036.8618 | MAE=4864.9868 | R2=0.9977\n",
            "Epoch 385 | Train MSE(scaled)=0.000493 | Val RMSE=5640.7903 | MAE=4699.0293 | R2=0.9980\n",
            "Epoch 386 | Train MSE(scaled)=0.000458 | Val RMSE=5531.9392 | MAE=4569.3511 | R2=0.9981\n",
            "Epoch 387 | Train MSE(scaled)=0.000429 | Val RMSE=5534.1580 | MAE=4561.9616 | R2=0.9981\n",
            "Epoch 388 | Train MSE(scaled)=0.000427 | Val RMSE=5408.7315 | MAE=4528.7906 | R2=0.9982\n",
            "Epoch 389 | Train MSE(scaled)=0.000501 | Val RMSE=5481.0064 | MAE=4540.6634 | R2=0.9981\n",
            "Epoch 390 | Train MSE(scaled)=0.000453 | Val RMSE=5556.6673 | MAE=4521.0684 | R2=0.9981\n",
            "Epoch 391 | Train MSE(scaled)=0.000423 | Val RMSE=5408.0448 | MAE=4426.2667 | R2=0.9982\n",
            "Epoch 392 | Train MSE(scaled)=0.000430 | Val RMSE=5490.5197 | MAE=4529.4003 | R2=0.9981\n",
            "Epoch 393 | Train MSE(scaled)=0.000442 | Val RMSE=5269.0040 | MAE=4379.7621 | R2=0.9983\n",
            "Epoch 394 | Train MSE(scaled)=0.000430 | Val RMSE=5450.7287 | MAE=4517.8022 | R2=0.9982\n",
            "Epoch 395 | Train MSE(scaled)=0.000413 | Val RMSE=5301.7850 | MAE=4387.1236 | R2=0.9983\n",
            "Epoch 396 | Train MSE(scaled)=0.000403 | Val RMSE=5284.0297 | MAE=4380.0847 | R2=0.9983\n",
            "Epoch 397 | Train MSE(scaled)=0.000412 | Val RMSE=5357.8466 | MAE=4408.4209 | R2=0.9982\n",
            "Epoch 398 | Train MSE(scaled)=0.000406 | Val RMSE=5132.5527 | MAE=4243.0825 | R2=0.9984\n",
            "Epoch 399 | Train MSE(scaled)=0.000424 | Val RMSE=5142.3113 | MAE=4267.5941 | R2=0.9984\n",
            "Epoch 400 | Train MSE(scaled)=0.000418 | Val RMSE=5244.8084 | MAE=4326.2919 | R2=0.9983\n",
            "Epoch 401 | Train MSE(scaled)=0.000412 | Val RMSE=5171.4849 | MAE=4320.9032 | R2=0.9983\n",
            "Epoch 402 | Train MSE(scaled)=0.000404 | Val RMSE=5018.2098 | MAE=4183.7212 | R2=0.9984\n",
            "Epoch 403 | Train MSE(scaled)=0.000423 | Val RMSE=5122.4965 | MAE=4251.5797 | R2=0.9984\n",
            "Epoch 404 | Train MSE(scaled)=0.000412 | Val RMSE=5197.5044 | MAE=4285.5343 | R2=0.9983\n",
            "Epoch 405 | Train MSE(scaled)=0.000423 | Val RMSE=4997.8550 | MAE=4090.9319 | R2=0.9985\n",
            "Epoch 406 | Train MSE(scaled)=0.000402 | Val RMSE=4996.5529 | MAE=4128.9162 | R2=0.9985\n",
            "Epoch 407 | Train MSE(scaled)=0.000408 | Val RMSE=5003.4683 | MAE=4126.4935 | R2=0.9984\n",
            "Epoch 408 | Train MSE(scaled)=0.000403 | Val RMSE=5020.0768 | MAE=4149.3958 | R2=0.9984\n",
            "Epoch 409 | Train MSE(scaled)=0.000409 | Val RMSE=4953.7956 | MAE=4053.3451 | R2=0.9985\n",
            "Epoch 410 | Train MSE(scaled)=0.000390 | Val RMSE=4984.5759 | MAE=4178.9391 | R2=0.9985\n",
            "Epoch 411 | Train MSE(scaled)=0.000414 | Val RMSE=4961.6713 | MAE=4080.8017 | R2=0.9985\n",
            "Epoch 412 | Train MSE(scaled)=0.000409 | Val RMSE=4829.9608 | MAE=3987.6430 | R2=0.9986\n",
            "Epoch 413 | Train MSE(scaled)=0.000390 | Val RMSE=4891.7276 | MAE=4021.8631 | R2=0.9985\n",
            "Epoch 414 | Train MSE(scaled)=0.000387 | Val RMSE=4888.9055 | MAE=4018.1536 | R2=0.9985\n",
            "Epoch 415 | Train MSE(scaled)=0.000392 | Val RMSE=4825.3837 | MAE=3967.0052 | R2=0.9986\n",
            "Epoch 416 | Train MSE(scaled)=0.000385 | Val RMSE=4744.6361 | MAE=3929.3653 | R2=0.9986\n",
            "Epoch 417 | Train MSE(scaled)=0.000404 | Val RMSE=4995.7631 | MAE=4074.0219 | R2=0.9985\n",
            "Epoch 418 | Train MSE(scaled)=0.000398 | Val RMSE=4785.3886 | MAE=3917.6778 | R2=0.9986\n",
            "Epoch 419 | Train MSE(scaled)=0.000358 | Val RMSE=4733.6371 | MAE=3881.0371 | R2=0.9986\n",
            "Epoch 420 | Train MSE(scaled)=0.000375 | Val RMSE=4984.1015 | MAE=4060.4100 | R2=0.9985\n",
            "Epoch 421 | Train MSE(scaled)=0.000383 | Val RMSE=4653.7991 | MAE=3795.8104 | R2=0.9987\n",
            "Epoch 422 | Train MSE(scaled)=0.000370 | Val RMSE=4675.2974 | MAE=3884.5607 | R2=0.9986\n",
            "Epoch 423 | Train MSE(scaled)=0.000376 | Val RMSE=4559.5418 | MAE=3814.7282 | R2=0.9987\n",
            "Epoch 424 | Train MSE(scaled)=0.000358 | Val RMSE=4647.1986 | MAE=3870.4418 | R2=0.9987\n",
            "Epoch 425 | Train MSE(scaled)=0.000360 | Val RMSE=4712.6203 | MAE=3904.5378 | R2=0.9986\n",
            "Epoch 426 | Train MSE(scaled)=0.000362 | Val RMSE=4614.9038 | MAE=3761.9244 | R2=0.9987\n",
            "Epoch 427 | Train MSE(scaled)=0.000378 | Val RMSE=4611.9043 | MAE=3857.7500 | R2=0.9987\n",
            "Epoch 428 | Train MSE(scaled)=0.000378 | Val RMSE=4566.7180 | MAE=3791.4688 | R2=0.9987\n",
            "Epoch 429 | Train MSE(scaled)=0.000377 | Val RMSE=4387.8803 | MAE=3650.7263 | R2=0.9988\n",
            "Epoch 430 | Train MSE(scaled)=0.000366 | Val RMSE=4525.6567 | MAE=3679.4881 | R2=0.9987\n",
            "Epoch 431 | Train MSE(scaled)=0.000366 | Val RMSE=4482.0967 | MAE=3727.0868 | R2=0.9988\n",
            "Epoch 432 | Train MSE(scaled)=0.000363 | Val RMSE=4369.1517 | MAE=3662.2600 | R2=0.9988\n",
            "Epoch 433 | Train MSE(scaled)=0.000385 | Val RMSE=4353.4178 | MAE=3586.5897 | R2=0.9988\n",
            "Epoch 434 | Train MSE(scaled)=0.000415 | Val RMSE=4489.5534 | MAE=3681.7625 | R2=0.9988\n",
            "Epoch 435 | Train MSE(scaled)=0.000385 | Val RMSE=4481.8971 | MAE=3732.4227 | R2=0.9988\n",
            "Epoch 436 | Train MSE(scaled)=0.000330 | Val RMSE=4279.9042 | MAE=3575.5637 | R2=0.9989\n",
            "Epoch 437 | Train MSE(scaled)=0.000300 | Val RMSE=4366.7554 | MAE=3589.6658 | R2=0.9988\n",
            "Epoch 438 | Train MSE(scaled)=0.000311 | Val RMSE=4210.8328 | MAE=3521.8533 | R2=0.9989\n",
            "Epoch 439 | Train MSE(scaled)=0.000309 | Val RMSE=4191.9710 | MAE=3507.0246 | R2=0.9989\n",
            "Epoch 440 | Train MSE(scaled)=0.000320 | Val RMSE=4225.9116 | MAE=3519.1116 | R2=0.9989\n",
            "Epoch 441 | Train MSE(scaled)=0.000296 | Val RMSE=4102.5568 | MAE=3391.5091 | R2=0.9990\n",
            "Epoch 442 | Train MSE(scaled)=0.000324 | Val RMSE=4082.3102 | MAE=3370.1555 | R2=0.9990\n",
            "Epoch 443 | Train MSE(scaled)=0.000323 | Val RMSE=4127.9215 | MAE=3426.4416 | R2=0.9989\n",
            "Epoch 444 | Train MSE(scaled)=0.000288 | Val RMSE=4002.1868 | MAE=3341.9334 | R2=0.9990\n",
            "Epoch 445 | Train MSE(scaled)=0.000289 | Val RMSE=4102.8345 | MAE=3451.4579 | R2=0.9990\n",
            "Epoch 446 | Train MSE(scaled)=0.000283 | Val RMSE=3987.8963 | MAE=3247.1874 | R2=0.9990\n",
            "Epoch 447 | Train MSE(scaled)=0.000303 | Val RMSE=4019.2243 | MAE=3414.1789 | R2=0.9990\n",
            "Epoch 448 | Train MSE(scaled)=0.000294 | Val RMSE=3950.3725 | MAE=3232.8776 | R2=0.9990\n",
            "Epoch 449 | Train MSE(scaled)=0.000296 | Val RMSE=3968.6639 | MAE=3236.2748 | R2=0.9990\n",
            "Epoch 450 | Train MSE(scaled)=0.000294 | Val RMSE=3883.9753 | MAE=3258.4281 | R2=0.9991\n",
            "Epoch 451 | Train MSE(scaled)=0.000286 | Val RMSE=3963.1966 | MAE=3201.3411 | R2=0.9990\n",
            "Epoch 452 | Train MSE(scaled)=0.000295 | Val RMSE=3787.5004 | MAE=3140.2858 | R2=0.9991\n",
            "Epoch 453 | Train MSE(scaled)=0.000313 | Val RMSE=4107.3161 | MAE=3331.1486 | R2=0.9990\n",
            "Epoch 454 | Train MSE(scaled)=0.000325 | Val RMSE=3881.1424 | MAE=3256.4738 | R2=0.9991\n",
            "Epoch 455 | Train MSE(scaled)=0.000312 | Val RMSE=3757.2066 | MAE=3083.2538 | R2=0.9991\n",
            "Epoch 456 | Train MSE(scaled)=0.000297 | Val RMSE=3747.2147 | MAE=3048.8554 | R2=0.9991\n",
            "Epoch 457 | Train MSE(scaled)=0.000284 | Val RMSE=3727.4906 | MAE=3086.0009 | R2=0.9991\n",
            "Epoch 458 | Train MSE(scaled)=0.000274 | Val RMSE=3675.4098 | MAE=3017.3007 | R2=0.9992\n",
            "Epoch 459 | Train MSE(scaled)=0.000266 | Val RMSE=3660.0896 | MAE=2989.7931 | R2=0.9992\n",
            "Epoch 460 | Train MSE(scaled)=0.000253 | Val RMSE=3756.4889 | MAE=3023.9449 | R2=0.9991\n",
            "Epoch 461 | Train MSE(scaled)=0.000265 | Val RMSE=3640.2283 | MAE=2969.0492 | R2=0.9992\n",
            "Epoch 462 | Train MSE(scaled)=0.000267 | Val RMSE=3675.9314 | MAE=2932.1710 | R2=0.9992\n",
            "Epoch 463 | Train MSE(scaled)=0.000266 | Val RMSE=3587.3193 | MAE=2913.0111 | R2=0.9992\n",
            "Epoch 464 | Train MSE(scaled)=0.000268 | Val RMSE=3511.2064 | MAE=2911.6555 | R2=0.9992\n",
            "Epoch 465 | Train MSE(scaled)=0.000258 | Val RMSE=3516.9549 | MAE=2883.0112 | R2=0.9992\n",
            "Epoch 466 | Train MSE(scaled)=0.000244 | Val RMSE=3554.5795 | MAE=2880.7063 | R2=0.9992\n",
            "Epoch 467 | Train MSE(scaled)=0.000261 | Val RMSE=3558.2325 | MAE=2879.6355 | R2=0.9992\n",
            "Epoch 468 | Train MSE(scaled)=0.000259 | Val RMSE=3504.7031 | MAE=2836.9533 | R2=0.9992\n",
            "Epoch 469 | Train MSE(scaled)=0.000263 | Val RMSE=3387.1803 | MAE=2727.8160 | R2=0.9993\n",
            "Epoch 470 | Train MSE(scaled)=0.000248 | Val RMSE=3315.1424 | MAE=2627.5286 | R2=0.9993\n",
            "Epoch 471 | Train MSE(scaled)=0.000258 | Val RMSE=3483.6461 | MAE=2794.6992 | R2=0.9992\n",
            "Epoch 472 | Train MSE(scaled)=0.000250 | Val RMSE=3302.3708 | MAE=2676.8313 | R2=0.9993\n",
            "Epoch 473 | Train MSE(scaled)=0.000262 | Val RMSE=3378.6782 | MAE=2689.8144 | R2=0.9993\n",
            "Epoch 474 | Train MSE(scaled)=0.000240 | Val RMSE=3276.7471 | MAE=2648.2905 | R2=0.9993\n",
            "Epoch 475 | Train MSE(scaled)=0.000249 | Val RMSE=3294.9354 | MAE=2622.0877 | R2=0.9993\n",
            "Epoch 476 | Train MSE(scaled)=0.000253 | Val RMSE=3393.1297 | MAE=2806.5653 | R2=0.9993\n",
            "Epoch 477 | Train MSE(scaled)=0.000257 | Val RMSE=3270.2534 | MAE=2544.7649 | R2=0.9993\n",
            "Epoch 478 | Train MSE(scaled)=0.000248 | Val RMSE=3298.9184 | MAE=2636.9414 | R2=0.9993\n",
            "Epoch 479 | Train MSE(scaled)=0.000260 | Val RMSE=3287.7911 | MAE=2698.1647 | R2=0.9993\n",
            "Epoch 480 | Train MSE(scaled)=0.000246 | Val RMSE=3264.1405 | MAE=2509.6701 | R2=0.9993\n",
            "Epoch 481 | Train MSE(scaled)=0.000254 | Val RMSE=3111.2422 | MAE=2509.6709 | R2=0.9994\n",
            "Epoch 482 | Train MSE(scaled)=0.000222 | Val RMSE=3201.4963 | MAE=2583.6637 | R2=0.9994\n",
            "Epoch 483 | Train MSE(scaled)=0.000236 | Val RMSE=3199.8798 | MAE=2584.3009 | R2=0.9994\n",
            "Epoch 484 | Train MSE(scaled)=0.000221 | Val RMSE=3014.4612 | MAE=2402.6984 | R2=0.9994\n",
            "Epoch 485 | Train MSE(scaled)=0.000212 | Val RMSE=2987.9776 | MAE=2340.9376 | R2=0.9994\n",
            "Epoch 486 | Train MSE(scaled)=0.000208 | Val RMSE=3008.7387 | MAE=2369.8875 | R2=0.9994\n",
            "Epoch 487 | Train MSE(scaled)=0.000207 | Val RMSE=2983.7124 | MAE=2420.8803 | R2=0.9994\n",
            "Epoch 488 | Train MSE(scaled)=0.000225 | Val RMSE=2954.6739 | MAE=2287.3711 | R2=0.9995\n",
            "Epoch 489 | Train MSE(scaled)=0.000193 | Val RMSE=2998.9678 | MAE=2397.6993 | R2=0.9994\n",
            "Epoch 490 | Train MSE(scaled)=0.000200 | Val RMSE=3072.4499 | MAE=2546.4966 | R2=0.9994\n",
            "Epoch 491 | Train MSE(scaled)=0.000208 | Val RMSE=2914.3898 | MAE=2285.2940 | R2=0.9995\n",
            "Epoch 492 | Train MSE(scaled)=0.000220 | Val RMSE=3038.9068 | MAE=2310.8680 | R2=0.9994\n",
            "Epoch 493 | Train MSE(scaled)=0.000208 | Val RMSE=2930.8189 | MAE=2382.3836 | R2=0.9995\n",
            "Epoch 494 | Train MSE(scaled)=0.000203 | Val RMSE=2912.0961 | MAE=2363.2686 | R2=0.9995\n",
            "Epoch 495 | Train MSE(scaled)=0.000193 | Val RMSE=2900.3443 | MAE=2210.7677 | R2=0.9995\n",
            "Epoch 496 | Train MSE(scaled)=0.000199 | Val RMSE=3007.0057 | MAE=2291.9696 | R2=0.9994\n",
            "Epoch 497 | Train MSE(scaled)=0.000221 | Val RMSE=2851.7058 | MAE=2272.1771 | R2=0.9995\n",
            "Epoch 498 | Train MSE(scaled)=0.000187 | Val RMSE=2888.5000 | MAE=2255.5617 | R2=0.9995\n",
            "Epoch 499 | Train MSE(scaled)=0.000200 | Val RMSE=2794.0075 | MAE=2203.0312 | R2=0.9995\n",
            "Epoch 500 | Train MSE(scaled)=0.000191 | Val RMSE=2932.5112 | MAE=2224.3040 | R2=0.9995\n",
            "Epoch 501 | Train MSE(scaled)=0.000213 | Val RMSE=2969.9478 | MAE=2377.6161 | R2=0.9995\n",
            "Epoch 502 | Train MSE(scaled)=0.000231 | Val RMSE=2904.7664 | MAE=2324.7472 | R2=0.9995\n",
            "Epoch 503 | Train MSE(scaled)=0.000194 | Val RMSE=2825.2795 | MAE=2172.2761 | R2=0.9995\n",
            "Epoch 504 | Train MSE(scaled)=0.000183 | Val RMSE=2743.0976 | MAE=2115.9567 | R2=0.9995\n",
            "Epoch 505 | Train MSE(scaled)=0.000188 | Val RMSE=2916.4543 | MAE=2279.6574 | R2=0.9995\n",
            "Epoch 506 | Train MSE(scaled)=0.000187 | Val RMSE=2699.4617 | MAE=2170.5721 | R2=0.9995\n",
            "Epoch 507 | Train MSE(scaled)=0.000186 | Val RMSE=2726.3660 | MAE=2067.6756 | R2=0.9995\n",
            "Epoch 508 | Train MSE(scaled)=0.000172 | Val RMSE=2819.3303 | MAE=2197.1103 | R2=0.9995\n",
            "Epoch 509 | Train MSE(scaled)=0.000179 | Val RMSE=2648.9306 | MAE=2096.1343 | R2=0.9996\n",
            "Epoch 510 | Train MSE(scaled)=0.000178 | Val RMSE=2544.2704 | MAE=2030.9240 | R2=0.9996\n",
            "Epoch 511 | Train MSE(scaled)=0.000203 | Val RMSE=2650.1231 | MAE=2109.6601 | R2=0.9996\n",
            "Epoch 512 | Train MSE(scaled)=0.000210 | Val RMSE=2774.5909 | MAE=2153.9194 | R2=0.9995\n",
            "Epoch 513 | Train MSE(scaled)=0.000180 | Val RMSE=2736.0439 | MAE=2076.3735 | R2=0.9995\n",
            "Epoch 514 | Train MSE(scaled)=0.000182 | Val RMSE=2495.6767 | MAE=1920.6910 | R2=0.9996\n",
            "Epoch 515 | Train MSE(scaled)=0.000180 | Val RMSE=2680.8823 | MAE=2082.4583 | R2=0.9996\n",
            "Epoch 516 | Train MSE(scaled)=0.000169 | Val RMSE=2708.9271 | MAE=2168.4448 | R2=0.9995\n",
            "Epoch 517 | Train MSE(scaled)=0.000158 | Val RMSE=2639.1369 | MAE=2064.6929 | R2=0.9996\n",
            "Epoch 518 | Train MSE(scaled)=0.000155 | Val RMSE=2509.4696 | MAE=1907.8559 | R2=0.9996\n",
            "Epoch 519 | Train MSE(scaled)=0.000164 | Val RMSE=2617.6183 | MAE=2019.8814 | R2=0.9996\n",
            "Epoch 520 | Train MSE(scaled)=0.000161 | Val RMSE=2430.0714 | MAE=1913.6396 | R2=0.9996\n",
            "Epoch 521 | Train MSE(scaled)=0.000156 | Val RMSE=2515.4758 | MAE=1960.4500 | R2=0.9996\n",
            "Epoch 522 | Train MSE(scaled)=0.000176 | Val RMSE=2384.2169 | MAE=1892.7709 | R2=0.9996\n",
            "Epoch 523 | Train MSE(scaled)=0.000175 | Val RMSE=2662.8607 | MAE=2098.2153 | R2=0.9996\n",
            "Epoch 524 | Train MSE(scaled)=0.000190 | Val RMSE=2527.0966 | MAE=1980.2348 | R2=0.9996\n",
            "Epoch 525 | Train MSE(scaled)=0.000179 | Val RMSE=2494.3331 | MAE=1950.6224 | R2=0.9996\n",
            "Epoch 526 | Train MSE(scaled)=0.000179 | Val RMSE=2491.6304 | MAE=1943.1060 | R2=0.9996\n",
            "Epoch 527 | Train MSE(scaled)=0.000162 | Val RMSE=2480.1169 | MAE=1864.7037 | R2=0.9996\n",
            "Epoch 528 | Train MSE(scaled)=0.000165 | Val RMSE=2409.8994 | MAE=1835.8937 | R2=0.9996\n",
            "Epoch 529 | Train MSE(scaled)=0.000150 | Val RMSE=2440.6340 | MAE=1817.1286 | R2=0.9996\n",
            "Epoch 530 | Train MSE(scaled)=0.000160 | Val RMSE=2399.0605 | MAE=1842.6608 | R2=0.9996\n",
            "Epoch 531 | Train MSE(scaled)=0.000159 | Val RMSE=2268.6032 | MAE=1675.2518 | R2=0.9997\n",
            "Epoch 532 | Train MSE(scaled)=0.000178 | Val RMSE=2348.5434 | MAE=1801.7474 | R2=0.9997\n",
            "Epoch 533 | Train MSE(scaled)=0.000175 | Val RMSE=2534.8491 | MAE=2038.7638 | R2=0.9996\n",
            "Epoch 534 | Train MSE(scaled)=0.000196 | Val RMSE=2384.0985 | MAE=1816.1166 | R2=0.9996\n",
            "Epoch 535 | Train MSE(scaled)=0.000164 | Val RMSE=2473.9315 | MAE=1832.2718 | R2=0.9996\n",
            "Epoch 536 | Train MSE(scaled)=0.000152 | Val RMSE=2503.5779 | MAE=1909.4031 | R2=0.9996\n",
            "Epoch 537 | Train MSE(scaled)=0.000179 | Val RMSE=2367.1537 | MAE=1870.0324 | R2=0.9997\n",
            "Epoch 538 | Train MSE(scaled)=0.000165 | Val RMSE=2360.9357 | MAE=1816.9761 | R2=0.9997\n",
            "Epoch 539 | Train MSE(scaled)=0.000174 | Val RMSE=2266.6447 | MAE=1797.6063 | R2=0.9997\n",
            "Epoch 540 | Train MSE(scaled)=0.000177 | Val RMSE=2511.6471 | MAE=1882.6913 | R2=0.9996\n",
            "Epoch 541 | Train MSE(scaled)=0.000160 | Val RMSE=2356.5657 | MAE=1818.1355 | R2=0.9997\n",
            "Epoch 542 | Train MSE(scaled)=0.000172 | Val RMSE=2263.7118 | MAE=1742.0281 | R2=0.9997\n",
            "Epoch 543 | Train MSE(scaled)=0.000197 | Val RMSE=2297.1964 | MAE=1774.6678 | R2=0.9997\n",
            "Epoch 544 | Train MSE(scaled)=0.000203 | Val RMSE=2572.1017 | MAE=1928.4764 | R2=0.9996\n",
            "Epoch 545 | Train MSE(scaled)=0.000169 | Val RMSE=2298.7995 | MAE=1802.7495 | R2=0.9997\n",
            "Epoch 546 | Train MSE(scaled)=0.000153 | Val RMSE=2161.5240 | MAE=1675.1329 | R2=0.9997\n",
            "Epoch 547 | Train MSE(scaled)=0.000161 | Val RMSE=2165.2436 | MAE=1706.7059 | R2=0.9997\n",
            "Epoch 548 | Train MSE(scaled)=0.000155 | Val RMSE=2334.0345 | MAE=1826.4564 | R2=0.9997\n",
            "Epoch 549 | Train MSE(scaled)=0.000166 | Val RMSE=2449.9831 | MAE=1780.2979 | R2=0.9996\n",
            "Epoch 550 | Train MSE(scaled)=0.000147 | Val RMSE=2177.7595 | MAE=1650.9535 | R2=0.9997\n",
            "Epoch 551 | Train MSE(scaled)=0.000152 | Val RMSE=2210.8392 | MAE=1748.3369 | R2=0.9997\n",
            "Epoch 552 | Train MSE(scaled)=0.000144 | Val RMSE=2271.9375 | MAE=1684.6434 | R2=0.9997\n",
            "Epoch 553 | Train MSE(scaled)=0.000147 | Val RMSE=2221.1781 | MAE=1689.9282 | R2=0.9997\n",
            "Epoch 554 | Train MSE(scaled)=0.000173 | Val RMSE=2443.9206 | MAE=1854.4183 | R2=0.9996\n",
            "Epoch 555 | Train MSE(scaled)=0.000172 | Val RMSE=2241.3603 | MAE=1652.0360 | R2=0.9997\n",
            "Epoch 556 | Train MSE(scaled)=0.000157 | Val RMSE=2137.4224 | MAE=1554.4360 | R2=0.9997\n",
            "Epoch 557 | Train MSE(scaled)=0.000132 | Val RMSE=2052.4673 | MAE=1601.2607 | R2=0.9997\n",
            "Epoch 558 | Train MSE(scaled)=0.000148 | Val RMSE=2090.2137 | MAE=1555.9728 | R2=0.9997\n",
            "Epoch 559 | Train MSE(scaled)=0.000141 | Val RMSE=2414.0422 | MAE=1828.5112 | R2=0.9996\n",
            "Epoch 560 | Train MSE(scaled)=0.000144 | Val RMSE=2110.2632 | MAE=1633.5312 | R2=0.9997\n",
            "Epoch 561 | Train MSE(scaled)=0.000146 | Val RMSE=2168.3885 | MAE=1526.3549 | R2=0.9997\n",
            "Epoch 562 | Train MSE(scaled)=0.000144 | Val RMSE=2222.9316 | MAE=1694.4818 | R2=0.9997\n",
            "Epoch 563 | Train MSE(scaled)=0.000140 | Val RMSE=2092.7778 | MAE=1593.1723 | R2=0.9997\n",
            "Epoch 564 | Train MSE(scaled)=0.000123 | Val RMSE=2079.9518 | MAE=1541.4652 | R2=0.9997\n",
            "Epoch 565 | Train MSE(scaled)=0.000127 | Val RMSE=2122.8522 | MAE=1621.1856 | R2=0.9997\n",
            "Epoch 566 | Train MSE(scaled)=0.000125 | Val RMSE=1994.8758 | MAE=1440.3749 | R2=0.9998\n",
            "Epoch 567 | Train MSE(scaled)=0.000130 | Val RMSE=1975.2248 | MAE=1455.1780 | R2=0.9998\n",
            "Epoch 568 | Train MSE(scaled)=0.000146 | Val RMSE=2106.2290 | MAE=1520.2664 | R2=0.9997\n",
            "Epoch 569 | Train MSE(scaled)=0.000159 | Val RMSE=2400.4058 | MAE=1899.2947 | R2=0.9996\n",
            "Epoch 570 | Train MSE(scaled)=0.000159 | Val RMSE=2118.4754 | MAE=1550.7886 | R2=0.9997\n",
            "Epoch 571 | Train MSE(scaled)=0.000133 | Val RMSE=2253.9417 | MAE=1685.8075 | R2=0.9997\n",
            "Epoch 572 | Train MSE(scaled)=0.000152 | Val RMSE=2419.6527 | MAE=1958.8983 | R2=0.9996\n",
            "Epoch 573 | Train MSE(scaled)=0.000183 | Val RMSE=2146.4847 | MAE=1503.5803 | R2=0.9997\n",
            "Epoch 574 | Train MSE(scaled)=0.000146 | Val RMSE=2106.7225 | MAE=1567.8921 | R2=0.9997\n",
            "Epoch 575 | Train MSE(scaled)=0.000143 | Val RMSE=2369.8147 | MAE=1808.8334 | R2=0.9997\n",
            "Epoch 576 | Train MSE(scaled)=0.000145 | Val RMSE=2129.6027 | MAE=1540.0359 | R2=0.9997\n",
            "Epoch 577 | Train MSE(scaled)=0.000129 | Val RMSE=1939.0140 | MAE=1436.4946 | R2=0.9998\n",
            "Epoch 578 | Train MSE(scaled)=0.000128 | Val RMSE=1986.2686 | MAE=1446.2137 | R2=0.9998\n",
            "Epoch 579 | Train MSE(scaled)=0.000114 | Val RMSE=1964.3076 | MAE=1431.3093 | R2=0.9998\n",
            "Epoch 580 | Train MSE(scaled)=0.000118 | Val RMSE=1980.6869 | MAE=1408.8657 | R2=0.9998\n",
            "Epoch 581 | Train MSE(scaled)=0.000125 | Val RMSE=2002.7233 | MAE=1396.7556 | R2=0.9998\n",
            "Epoch 582 | Train MSE(scaled)=0.000155 | Val RMSE=2046.0534 | MAE=1586.0625 | R2=0.9997\n",
            "Epoch 583 | Train MSE(scaled)=0.000129 | Val RMSE=1940.1192 | MAE=1350.2179 | R2=0.9998\n",
            "Epoch 584 | Train MSE(scaled)=0.000117 | Val RMSE=1977.6172 | MAE=1408.5140 | R2=0.9998\n",
            "Epoch 585 | Train MSE(scaled)=0.000126 | Val RMSE=2236.3726 | MAE=1625.7003 | R2=0.9997\n",
            "Epoch 586 | Train MSE(scaled)=0.000135 | Val RMSE=2076.7584 | MAE=1525.9709 | R2=0.9997\n",
            "Epoch 587 | Train MSE(scaled)=0.000123 | Val RMSE=2155.0681 | MAE=1520.4872 | R2=0.9997\n",
            "Epoch 588 | Train MSE(scaled)=0.000135 | Val RMSE=2093.5188 | MAE=1538.6842 | R2=0.9997\n",
            "Epoch 589 | Train MSE(scaled)=0.000153 | Val RMSE=2084.0869 | MAE=1458.6742 | R2=0.9997\n",
            "Epoch 590 | Train MSE(scaled)=0.000142 | Val RMSE=1974.0778 | MAE=1376.0883 | R2=0.9998\n",
            "Epoch 591 | Train MSE(scaled)=0.000131 | Val RMSE=1998.8643 | MAE=1411.4144 | R2=0.9998\n",
            "Epoch 592 | Train MSE(scaled)=0.000109 | Val RMSE=1904.5072 | MAE=1400.4220 | R2=0.9998\n",
            "Epoch 593 | Train MSE(scaled)=0.000133 | Val RMSE=1940.0881 | MAE=1480.3796 | R2=0.9998\n",
            "Epoch 594 | Train MSE(scaled)=0.000126 | Val RMSE=2044.5036 | MAE=1399.3232 | R2=0.9997\n",
            "Epoch 595 | Train MSE(scaled)=0.000127 | Val RMSE=1890.1620 | MAE=1462.1780 | R2=0.9998\n",
            "Epoch 596 | Train MSE(scaled)=0.000187 | Val RMSE=2132.4051 | MAE=1485.2514 | R2=0.9997\n",
            "Epoch 597 | Train MSE(scaled)=0.000141 | Val RMSE=2094.5351 | MAE=1473.5131 | R2=0.9997\n",
            "Epoch 598 | Train MSE(scaled)=0.000123 | Val RMSE=1963.1494 | MAE=1491.2203 | R2=0.9998\n",
            "Epoch 599 | Train MSE(scaled)=0.000156 | Val RMSE=2070.5494 | MAE=1521.8403 | R2=0.9997\n",
            "Epoch 600 | Train MSE(scaled)=0.000137 | Val RMSE=1978.8078 | MAE=1390.5632 | R2=0.9998\n",
            "Epoch 601 | Train MSE(scaled)=0.000117 | Val RMSE=1905.4507 | MAE=1446.5912 | R2=0.9998\n",
            "Epoch 602 | Train MSE(scaled)=0.000127 | Val RMSE=1996.4913 | MAE=1430.1103 | R2=0.9998\n",
            "Epoch 603 | Train MSE(scaled)=0.000115 | Val RMSE=1876.5622 | MAE=1424.5779 | R2=0.9998\n",
            "Epoch 604 | Train MSE(scaled)=0.000120 | Val RMSE=1895.3309 | MAE=1382.1968 | R2=0.9998\n",
            "Epoch 605 | Train MSE(scaled)=0.000127 | Val RMSE=1955.0231 | MAE=1498.5913 | R2=0.9998\n",
            "Epoch 606 | Train MSE(scaled)=0.000135 | Val RMSE=1969.6380 | MAE=1388.2373 | R2=0.9998\n",
            "Epoch 607 | Train MSE(scaled)=0.000128 | Val RMSE=2171.4830 | MAE=1561.9714 | R2=0.9997\n",
            "Epoch 608 | Train MSE(scaled)=0.000156 | Val RMSE=2040.0579 | MAE=1511.6368 | R2=0.9997\n",
            "Epoch 609 | Train MSE(scaled)=0.000151 | Val RMSE=2313.2497 | MAE=1691.0204 | R2=0.9997\n",
            "Epoch 610 | Train MSE(scaled)=0.000155 | Val RMSE=2040.8235 | MAE=1618.7657 | R2=0.9997\n",
            "Epoch 611 | Train MSE(scaled)=0.000141 | Val RMSE=2296.0297 | MAE=1658.7077 | R2=0.9997\n",
            "Epoch 612 | Train MSE(scaled)=0.000135 | Val RMSE=1793.6579 | MAE=1333.3920 | R2=0.9998\n",
            "Epoch 613 | Train MSE(scaled)=0.000133 | Val RMSE=2042.9464 | MAE=1424.3307 | R2=0.9997\n",
            "Epoch 614 | Train MSE(scaled)=0.000174 | Val RMSE=1872.8083 | MAE=1372.9748 | R2=0.9998\n",
            "Epoch 615 | Train MSE(scaled)=0.000141 | Val RMSE=1984.0127 | MAE=1376.7881 | R2=0.9998\n",
            "Epoch 616 | Train MSE(scaled)=0.000131 | Val RMSE=2130.4174 | MAE=1506.0162 | R2=0.9997\n",
            "Epoch 617 | Train MSE(scaled)=0.000141 | Val RMSE=2076.4490 | MAE=1484.0739 | R2=0.9997\n",
            "Epoch 618 | Train MSE(scaled)=0.000134 | Val RMSE=1893.9515 | MAE=1270.2279 | R2=0.9998\n",
            "Epoch 619 | Train MSE(scaled)=0.000109 | Val RMSE=1734.8599 | MAE=1219.1409 | R2=0.9998\n",
            "Epoch 620 | Train MSE(scaled)=0.000129 | Val RMSE=2155.8156 | MAE=1567.1440 | R2=0.9997\n",
            "Epoch 621 | Train MSE(scaled)=0.000129 | Val RMSE=1853.2576 | MAE=1266.3500 | R2=0.9998\n",
            "Epoch 622 | Train MSE(scaled)=0.000107 | Val RMSE=2131.0793 | MAE=1593.2963 | R2=0.9997\n",
            "Epoch 623 | Train MSE(scaled)=0.000127 | Val RMSE=1912.8936 | MAE=1263.1117 | R2=0.9998\n",
            "Epoch 624 | Train MSE(scaled)=0.000121 | Val RMSE=1770.2153 | MAE=1249.8586 | R2=0.9998\n",
            "Epoch 625 | Train MSE(scaled)=0.000135 | Val RMSE=1897.4385 | MAE=1311.5608 | R2=0.9998\n",
            "Epoch 626 | Train MSE(scaled)=0.000121 | Val RMSE=1861.9423 | MAE=1306.7020 | R2=0.9998\n",
            "Epoch 627 | Train MSE(scaled)=0.000114 | Val RMSE=1767.9447 | MAE=1222.6155 | R2=0.9998\n",
            "Epoch 628 | Train MSE(scaled)=0.000107 | Val RMSE=1950.5663 | MAE=1409.3600 | R2=0.9998\n",
            "Epoch 629 | Train MSE(scaled)=0.000116 | Val RMSE=2163.9518 | MAE=1514.7129 | R2=0.9997\n",
            "Epoch 630 | Train MSE(scaled)=0.000118 | Val RMSE=1913.0911 | MAE=1429.5026 | R2=0.9998\n",
            "Epoch 631 | Train MSE(scaled)=0.000136 | Val RMSE=1929.3857 | MAE=1376.8049 | R2=0.9998\n",
            "Epoch 632 | Train MSE(scaled)=0.000148 | Val RMSE=2030.9317 | MAE=1347.9792 | R2=0.9997\n",
            "Epoch 633 | Train MSE(scaled)=0.000128 | Val RMSE=1923.2331 | MAE=1328.2204 | R2=0.9998\n",
            "Epoch 634 | Train MSE(scaled)=0.000124 | Val RMSE=1802.0209 | MAE=1231.3027 | R2=0.9998\n",
            "Epoch 635 | Train MSE(scaled)=0.000131 | Val RMSE=1819.1921 | MAE=1281.5212 | R2=0.9998\n",
            "Epoch 636 | Train MSE(scaled)=0.000128 | Val RMSE=1905.1101 | MAE=1361.4305 | R2=0.9998\n",
            "Epoch 637 | Train MSE(scaled)=0.000110 | Val RMSE=1697.7449 | MAE=1214.8897 | R2=0.9998\n",
            "Epoch 638 | Train MSE(scaled)=0.000121 | Val RMSE=2032.4686 | MAE=1638.5306 | R2=0.9997\n",
            "Epoch 639 | Train MSE(scaled)=0.000151 | Val RMSE=1791.5449 | MAE=1330.3956 | R2=0.9998\n",
            "Epoch 640 | Train MSE(scaled)=0.000123 | Val RMSE=1837.2307 | MAE=1311.9852 | R2=0.9998\n",
            "Epoch 641 | Train MSE(scaled)=0.000117 | Val RMSE=1707.1664 | MAE=1227.5228 | R2=0.9998\n",
            "Epoch 642 | Train MSE(scaled)=0.000113 | Val RMSE=1642.8451 | MAE=1135.4691 | R2=0.9998\n",
            "Epoch 643 | Train MSE(scaled)=0.000113 | Val RMSE=1675.6433 | MAE=1206.8265 | R2=0.9998\n",
            "Epoch 644 | Train MSE(scaled)=0.000124 | Val RMSE=1705.4389 | MAE=1210.7233 | R2=0.9998\n",
            "Epoch 645 | Train MSE(scaled)=0.000122 | Val RMSE=1736.5151 | MAE=1233.3093 | R2=0.9998\n",
            "Epoch 646 | Train MSE(scaled)=0.000122 | Val RMSE=1772.4665 | MAE=1296.3337 | R2=0.9998\n",
            "Epoch 647 | Train MSE(scaled)=0.000102 | Val RMSE=1746.6871 | MAE=1202.6409 | R2=0.9998\n",
            "Epoch 648 | Train MSE(scaled)=0.000123 | Val RMSE=1752.3563 | MAE=1257.6699 | R2=0.9998\n",
            "Epoch 649 | Train MSE(scaled)=0.000128 | Val RMSE=1786.8839 | MAE=1277.7902 | R2=0.9998\n",
            "Epoch 650 | Train MSE(scaled)=0.000152 | Val RMSE=2099.5464 | MAE=1550.9666 | R2=0.9997\n",
            "Epoch 651 | Train MSE(scaled)=0.000197 | Val RMSE=2368.4799 | MAE=1813.5258 | R2=0.9997\n",
            "Epoch 652 | Train MSE(scaled)=0.000249 | Val RMSE=2420.2219 | MAE=1958.4821 | R2=0.9996\n",
            "Epoch 653 | Train MSE(scaled)=0.000205 | Val RMSE=2033.0588 | MAE=1561.3556 | R2=0.9997\n",
            "Epoch 654 | Train MSE(scaled)=0.000138 | Val RMSE=1818.1718 | MAE=1417.5120 | R2=0.9998\n",
            "Epoch 655 | Train MSE(scaled)=0.000144 | Val RMSE=1982.0735 | MAE=1386.5517 | R2=0.9998\n",
            "Epoch 656 | Train MSE(scaled)=0.000164 | Val RMSE=2365.8013 | MAE=1933.8434 | R2=0.9997\n",
            "Epoch 657 | Train MSE(scaled)=0.000159 | Val RMSE=1794.5161 | MAE=1271.1199 | R2=0.9998\n",
            "Epoch 658 | Train MSE(scaled)=0.000123 | Val RMSE=1831.1147 | MAE=1395.4095 | R2=0.9998\n",
            "Epoch 659 | Train MSE(scaled)=0.000115 | Val RMSE=1717.6198 | MAE=1247.3013 | R2=0.9998\n",
            "Epoch 660 | Train MSE(scaled)=0.000100 | Val RMSE=1705.8284 | MAE=1261.6353 | R2=0.9998\n",
            "Epoch 661 | Train MSE(scaled)=0.000105 | Val RMSE=1802.8149 | MAE=1307.6691 | R2=0.9998\n",
            "Epoch 662 | Train MSE(scaled)=0.000107 | Val RMSE=1750.3062 | MAE=1224.2141 | R2=0.9998\n",
            "Early stopping at epoch 662. Best scaled RMSE=0.013149\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluate"
      ],
      "metadata": {
        "id": "vWdU7vtDx8kY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Forward test\n",
        "_, _, _, _, _, _, yhat_test_s = forward(X_test_s)\n",
        "\n",
        "# Thang gốc\n",
        "yhat_test_orig = scaler_y.inverse_transform(yhat_test_s)\n",
        "test_rmse = rmse(y_test, yhat_test_orig)\n",
        "test_mae  = mae(y_test, yhat_test_orig)\n",
        "test_r2   = r2_score(y_test, yhat_test_orig)\n",
        "\n",
        "print(f\"\\nTest — RMSE: {test_rmse:.4f} | MAE: {test_mae:.4f} | R2: {test_r2:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZUX7zB1lx-Ru",
        "outputId": "d49327b0-6560-4822-802d-0ddb6c16792b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test — RMSE: 1525.3599 | MAE: 1158.3417 | R2: 0.9999\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code using Tensorflow"
      ],
      "metadata": {
        "id": "6hhB9TK8UCRF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Libraries\n"
      ],
      "metadata": {
        "id": "McWZYE_QqyUe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, Sequential, regularizers\n",
        "from sklearn.metrics import r2_score\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "metadata": {
        "id": "AJLPsXsNq37s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Data"
      ],
      "metadata": {
        "id": "McRPxJVGrBMM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "df = pd.read_csv('/content/drive/MyDrive/train.csv')\n",
        "print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dgHPduZjrLcj",
        "outputId": "28731b17-1ec3-4565-e7d2-7bfb9c5c14c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "   v.id  on road old  on road now  years      km  rating  condition  economy  \\\n",
            "0     1       535651       798186      3   78945       1          2       14   \n",
            "1     2       591911       861056      6  117220       5          9        9   \n",
            "2     3       686990       770762      2  132538       2          8       15   \n",
            "3     4       573999       722381      4  101065       4          3       11   \n",
            "4     5       691388       811335      6   61559       3          9       12   \n",
            "\n",
            "   top speed  hp  torque  current price  \n",
            "0        177  73     123       351318.0  \n",
            "1        148  74      95       285001.5  \n",
            "2        181  53      97       215386.0  \n",
            "3        197  54     116       244295.5  \n",
            "4        160  53     105       531114.5  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = tf.convert_to_tensor(df.drop(df.columns[0], axis = 1))\n",
        "X = data[:, :-1].numpy()\n",
        "y = data[:, -1]\n",
        "y = tf.expand_dims(y, 1).numpy()\n",
        "X.shape, y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-t3L_XDormf9",
        "outputId": "772d9154-3bb5-4838-d527-bd08ae5422a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1000, 10), (1000, 1))"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare the data"
      ],
      "metadata": {
        "id": "fnTPcU1_uAe5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TRAIN_RATIO, VAL_RATIO = 0.8, 0.1\n",
        "BATCH_SIZE, SEED = 64, 123\n",
        "\n",
        "n = len(X)\n",
        "idx = tf.random.shuffle(tf.range(n), seed=42)\n",
        "n_train = int(n * TRAIN_RATIO)\n",
        "n_val   = int(n * VAL_RATIO)\n",
        "n_test  = n - n_train - n_val\n",
        "\n",
        "idx_train = idx[:n_train].numpy()\n",
        "idx_val   = idx[n_train:n_train+n_val].numpy()\n",
        "idx_test  = idx[n_train+n_val:].numpy()\n",
        "\n",
        "X_train, y_train = X[idx_train], y[idx_train]\n",
        "X_val,   y_val   = X[idx_val],   y[idx_val]\n",
        "X_test,  y_test  = X[idx_test],  y[idx_test]\n",
        "\n",
        "scaler_X = StandardScaler()\n",
        "X_train_s = scaler_X.fit_transform(X_train)\n",
        "X_val_s = scaler_X.transform(X_val)\n",
        "X_test_s = scaler_X.transform(X_test)\n",
        "\n",
        "scaler_y = StandardScaler()\n",
        "y_train_s = scaler_y.fit_transform(y_train)\n",
        "y_val_s = scaler_y.transform(y_val)\n",
        "y_test_s = scaler_y.transform(y_test)\n",
        "\n",
        "train_ds = (tf.data.Dataset.from_tensor_slices((X_train_s, y_train_s))\n",
        "            .shuffle(buffer_size=n_train, seed=SEED, reshuffle_each_iteration=True)\n",
        "            .batch(BATCH_SIZE)\n",
        "            .prefetch(tf.data.AUTOTUNE))\n",
        "\n",
        "val_ds  = (tf.data.Dataset.from_tensor_slices((X_val_s, y_val_s))\n",
        "           .batch(BATCH_SIZE)\n",
        "           .prefetch(tf.data.AUTOTUNE))\n",
        "\n",
        "test_ds = (tf.data.Dataset.from_tensor_slices((X_test_s, y_test_s))\n",
        "           .batch(BATCH_SIZE)\n",
        "           .prefetch(tf.data.AUTOTUNE))\n"
      ],
      "metadata": {
        "id": "7k2Ruv2WuCXy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "xfzaVddN3yCT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Weight decay thông qua L2 regularization (kernel_regularizer)\n",
        "l2_lambda = 1e-3\n",
        "drop1, drop2 = 0.1, 0.05\n",
        "in_dim = X_train_s.shape[1]\n",
        "\n",
        "model = Sequential([\n",
        "    layers.Input(shape=(in_dim,)),\n",
        "    layers.Dense(64, activation=\"relu\",\n",
        "                 kernel_regularizer=regularizers.l2(l2_lambda)),\n",
        "    layers.Dropout(drop1),\n",
        "    layers.Dense(32, activation=\"relu\",\n",
        "                 kernel_regularizer=regularizers.l2(l2_lambda)),\n",
        "    layers.Dropout(drop2),\n",
        "    layers.Dense(1)\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
        "    loss=\"mse\",\n",
        "    metrics=[keras.metrics.RootMeanSquaredError(name=\"rmse\")]\n",
        ")\n",
        "\n",
        "# Early stopping: dừng khi val_loss không cải thiện, khôi phục trọng số tốt nhất\n",
        "early_stop = keras.callbacks.EarlyStopping(\n",
        "    monitor=\"val_loss\",\n",
        "    mode=\"min\",\n",
        "    patience=20,\n",
        "    restore_best_weights=True\n",
        ")"
      ],
      "metadata": {
        "id": "ZKniAwsO34Km"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=500,            # đặt lớn, early stopping sẽ dừng sớm\n",
        "    callbacks=[early_stop],\n",
        "    verbose=1\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5LcxrZrf5cZT",
        "outputId": "e28c4852-d3cf-431b-9006-5a7f5d8a92bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - loss: 0.9235 - rmse: 0.9275 - val_loss: 0.5636 - val_rmse: 0.7098\n",
            "Epoch 2/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.5497 - rmse: 0.6986 - val_loss: 0.2803 - val_rmse: 0.4697\n",
            "Epoch 3/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2896 - rmse: 0.4789 - val_loss: 0.1383 - val_rmse: 0.2801\n",
            "Epoch 4/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2096 - rmse: 0.3864 - val_loss: 0.1029 - val_rmse: 0.2078\n",
            "Epoch 5/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1550 - rmse: 0.3089 - val_loss: 0.0918 - val_rmse: 0.1810\n",
            "Epoch 6/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1505 - rmse: 0.3027 - val_loss: 0.0828 - val_rmse: 0.1564\n",
            "Epoch 7/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1464 - rmse: 0.2970 - val_loss: 0.0796 - val_rmse: 0.1482\n",
            "Epoch 8/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.1370 - rmse: 0.2817 - val_loss: 0.0767 - val_rmse: 0.1403\n",
            "Epoch 9/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1270 - rmse: 0.2648 - val_loss: 0.0712 - val_rmse: 0.1220\n",
            "Epoch 10/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1212 - rmse: 0.2549 - val_loss: 0.0711 - val_rmse: 0.1243\n",
            "Epoch 11/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1300 - rmse: 0.2728 - val_loss: 0.0681 - val_rmse: 0.1148\n",
            "Epoch 12/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1180 - rmse: 0.2512 - val_loss: 0.0669 - val_rmse: 0.1125\n",
            "Epoch 13/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1121 - rmse: 0.2408 - val_loss: 0.0656 - val_rmse: 0.1096\n",
            "Epoch 14/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1114 - rmse: 0.2408 - val_loss: 0.0648 - val_rmse: 0.1088\n",
            "Epoch 15/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0979 - rmse: 0.2125 - val_loss: 0.0643 - val_rmse: 0.1097\n",
            "Epoch 16/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1008 - rmse: 0.2206 - val_loss: 0.0617 - val_rmse: 0.1002\n",
            "Epoch 17/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1083 - rmse: 0.2383 - val_loss: 0.0648 - val_rmse: 0.1177\n",
            "Epoch 18/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0949 - rmse: 0.2098 - val_loss: 0.0617 - val_rmse: 0.1062\n",
            "Epoch 19/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0929 - rmse: 0.2065 - val_loss: 0.0607 - val_rmse: 0.1045\n",
            "Epoch 20/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0897 - rmse: 0.2002 - val_loss: 0.0607 - val_rmse: 0.1079\n",
            "Epoch 21/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0901 - rmse: 0.2028 - val_loss: 0.0576 - val_rmse: 0.0951\n",
            "Epoch 22/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0889 - rmse: 0.2012 - val_loss: 0.0584 - val_rmse: 0.1021\n",
            "Epoch 23/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0846 - rmse: 0.1915 - val_loss: 0.0579 - val_rmse: 0.1027\n",
            "Epoch 24/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0859 - rmse: 0.1961 - val_loss: 0.0556 - val_rmse: 0.0939\n",
            "Epoch 25/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0799 - rmse: 0.1824 - val_loss: 0.0541 - val_rmse: 0.0888\n",
            "Epoch 26/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0845 - rmse: 0.1959 - val_loss: 0.0539 - val_rmse: 0.0906\n",
            "Epoch 27/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0819 - rmse: 0.1905 - val_loss: 0.0534 - val_rmse: 0.0911\n",
            "Epoch 28/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0769 - rmse: 0.1786 - val_loss: 0.0516 - val_rmse: 0.0839\n",
            "Epoch 29/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0741 - rmse: 0.1720 - val_loss: 0.0506 - val_rmse: 0.0809\n",
            "Epoch 30/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0784 - rmse: 0.1857 - val_loss: 0.0506 - val_rmse: 0.0840\n",
            "Epoch 31/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0794 - rmse: 0.1897 - val_loss: 0.0506 - val_rmse: 0.0872\n",
            "Epoch 32/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0706 - rmse: 0.1664 - val_loss: 0.0494 - val_rmse: 0.0829\n",
            "Epoch 33/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0747 - rmse: 0.1796 - val_loss: 0.0480 - val_rmse: 0.0772\n",
            "Epoch 34/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0685 - rmse: 0.1629 - val_loss: 0.0501 - val_rmse: 0.0929\n",
            "Epoch 35/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0674 - rmse: 0.1614 - val_loss: 0.0474 - val_rmse: 0.0798\n",
            "Epoch 36/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0691 - rmse: 0.1680 - val_loss: 0.0479 - val_rmse: 0.0860\n",
            "Epoch 37/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0718 - rmse: 0.1770 - val_loss: 0.0473 - val_rmse: 0.0854\n",
            "Epoch 38/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0672 - rmse: 0.1648 - val_loss: 0.0454 - val_rmse: 0.0764\n",
            "Epoch 39/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0661 - rmse: 0.1632 - val_loss: 0.0461 - val_rmse: 0.0836\n",
            "Epoch 40/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0728 - rmse: 0.1831 - val_loss: 0.0439 - val_rmse: 0.0724\n",
            "Epoch 41/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0625 - rmse: 0.1544 - val_loss: 0.0449 - val_rmse: 0.0819\n",
            "Epoch 42/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0654 - rmse: 0.1652 - val_loss: 0.0439 - val_rmse: 0.0784\n",
            "Epoch 43/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0617 - rmse: 0.1550 - val_loss: 0.0429 - val_rmse: 0.0749\n",
            "Epoch 44/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0590 - rmse: 0.1478 - val_loss: 0.0445 - val_rmse: 0.0870\n",
            "Epoch 45/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0610 - rmse: 0.1555 - val_loss: 0.0419 - val_rmse: 0.0735\n",
            "Epoch 46/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0644 - rmse: 0.1668 - val_loss: 0.0421 - val_rmse: 0.0778\n",
            "Epoch 47/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0619 - rmse: 0.1608 - val_loss: 0.0404 - val_rmse: 0.0694\n",
            "Epoch 48/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0578 - rmse: 0.1493 - val_loss: 0.0406 - val_rmse: 0.0732\n",
            "Epoch 49/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0553 - rmse: 0.1421 - val_loss: 0.0396 - val_rmse: 0.0691\n",
            "Epoch 50/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0551 - rmse: 0.1426 - val_loss: 0.0392 - val_rmse: 0.0696\n",
            "Epoch 51/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0528 - rmse: 0.1358 - val_loss: 0.0392 - val_rmse: 0.0724\n",
            "Epoch 52/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0535 - rmse: 0.1398 - val_loss: 0.0384 - val_rmse: 0.0687\n",
            "Epoch 53/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0533 - rmse: 0.1408 - val_loss: 0.0382 - val_rmse: 0.0705\n",
            "Epoch 54/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0506 - rmse: 0.1322 - val_loss: 0.0377 - val_rmse: 0.0698\n",
            "Epoch 55/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0537 - rmse: 0.1446 - val_loss: 0.0370 - val_rmse: 0.0679\n",
            "Epoch 56/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0511 - rmse: 0.1370 - val_loss: 0.0377 - val_rmse: 0.0750\n",
            "Epoch 57/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0494 - rmse: 0.1322 - val_loss: 0.0360 - val_rmse: 0.0655\n",
            "Epoch 58/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0499 - rmse: 0.1353 - val_loss: 0.0357 - val_rmse: 0.0659\n",
            "Epoch 59/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0489 - rmse: 0.1329 - val_loss: 0.0354 - val_rmse: 0.0668\n",
            "Epoch 60/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0510 - rmse: 0.1418 - val_loss: 0.0362 - val_rmse: 0.0747\n",
            "Epoch 61/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0490 - rmse: 0.1361 - val_loss: 0.0351 - val_rmse: 0.0696\n",
            "Epoch 62/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0477 - rmse: 0.1325 - val_loss: 0.0345 - val_rmse: 0.0682\n",
            "Epoch 63/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0457 - rmse: 0.1261 - val_loss: 0.0342 - val_rmse: 0.0681\n",
            "Epoch 64/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0468 - rmse: 0.1316 - val_loss: 0.0337 - val_rmse: 0.0674\n",
            "Epoch 65/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0457 - rmse: 0.1286 - val_loss: 0.0331 - val_rmse: 0.0649\n",
            "Epoch 66/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0469 - rmse: 0.1347 - val_loss: 0.0333 - val_rmse: 0.0690\n",
            "Epoch 67/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0479 - rmse: 0.1392 - val_loss: 0.0320 - val_rmse: 0.0616\n",
            "Epoch 68/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0445 - rmse: 0.1278 - val_loss: 0.0331 - val_rmse: 0.0726\n",
            "Epoch 69/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0426 - rmse: 0.1216 - val_loss: 0.0319 - val_rmse: 0.0654\n",
            "Epoch 70/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0432 - rmse: 0.1252 - val_loss: 0.0330 - val_rmse: 0.0756\n",
            "Epoch 71/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0427 - rmse: 0.1244 - val_loss: 0.0311 - val_rmse: 0.0648\n",
            "Epoch 72/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0431 - rmse: 0.1275 - val_loss: 0.0316 - val_rmse: 0.0709\n",
            "Epoch 73/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0446 - rmse: 0.1344 - val_loss: 0.0300 - val_rmse: 0.0607\n",
            "Epoch 74/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0435 - rmse: 0.1311 - val_loss: 0.0314 - val_rmse: 0.0735\n",
            "Epoch 75/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0408 - rmse: 0.1217 - val_loss: 0.0297 - val_rmse: 0.0630\n",
            "Epoch 76/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0451 - rmse: 0.1393 - val_loss: 0.0301 - val_rmse: 0.0685\n",
            "Epoch 77/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0399 - rmse: 0.1204 - val_loss: 0.0292 - val_rmse: 0.0637\n",
            "Epoch 78/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0436 - rmse: 0.1359 - val_loss: 0.0288 - val_rmse: 0.0632\n",
            "Epoch 79/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0387 - rmse: 0.1179 - val_loss: 0.0289 - val_rmse: 0.0658\n",
            "Epoch 80/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0380 - rmse: 0.1161 - val_loss: 0.0289 - val_rmse: 0.0680\n",
            "Epoch 81/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0381 - rmse: 0.1180 - val_loss: 0.0279 - val_rmse: 0.0629\n",
            "Epoch 82/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0400 - rmse: 0.1267 - val_loss: 0.0279 - val_rmse: 0.0649\n",
            "Epoch 83/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0396 - rmse: 0.1261 - val_loss: 0.0285 - val_rmse: 0.0712\n",
            "Epoch 84/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0377 - rmse: 0.1199 - val_loss: 0.0275 - val_rmse: 0.0658\n",
            "Epoch 85/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0378 - rmse: 0.1212 - val_loss: 0.0265 - val_rmse: 0.0601\n",
            "Epoch 86/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0356 - rmse: 0.1131 - val_loss: 0.0262 - val_rmse: 0.0600\n",
            "Epoch 87/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0377 - rmse: 0.1229 - val_loss: 0.0255 - val_rmse: 0.0560\n",
            "Epoch 88/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0383 - rmse: 0.1267 - val_loss: 0.0257 - val_rmse: 0.0602\n",
            "Epoch 89/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0366 - rmse: 0.1209 - val_loss: 0.0253 - val_rmse: 0.0594\n",
            "Epoch 90/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0347 - rmse: 0.1134 - val_loss: 0.0251 - val_rmse: 0.0591\n",
            "Epoch 91/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0346 - rmse: 0.1144 - val_loss: 0.0272 - val_rmse: 0.0770\n",
            "Epoch 92/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0382 - rmse: 0.1301 - val_loss: 0.0242 - val_rmse: 0.0555\n",
            "Epoch 93/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0356 - rmse: 0.1207 - val_loss: 0.0250 - val_rmse: 0.0640\n",
            "Epoch 94/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0329 - rmse: 0.1100 - val_loss: 0.0241 - val_rmse: 0.0595\n",
            "Epoch 95/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0364 - rmse: 0.1255 - val_loss: 0.0234 - val_rmse: 0.0550\n",
            "Epoch 96/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0331 - rmse: 0.1132 - val_loss: 0.0241 - val_rmse: 0.0629\n",
            "Epoch 97/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0332 - rmse: 0.1144 - val_loss: 0.0226 - val_rmse: 0.0516\n",
            "Epoch 98/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0305 - rmse: 0.1029 - val_loss: 0.0227 - val_rmse: 0.0550\n",
            "Epoch 99/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0321 - rmse: 0.1115 - val_loss: 0.0233 - val_rmse: 0.0620\n",
            "Epoch 100/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0322 - rmse: 0.1130 - val_loss: 0.0223 - val_rmse: 0.0556\n",
            "Epoch 101/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0318 - rmse: 0.1125 - val_loss: 0.0231 - val_rmse: 0.0642\n",
            "Epoch 102/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0307 - rmse: 0.1081 - val_loss: 0.0221 - val_rmse: 0.0578\n",
            "Epoch 103/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0301 - rmse: 0.1063 - val_loss: 0.0218 - val_rmse: 0.0562\n",
            "Epoch 104/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0330 - rmse: 0.1202 - val_loss: 0.0217 - val_rmse: 0.0569\n",
            "Epoch 105/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0295 - rmse: 0.1055 - val_loss: 0.0211 - val_rmse: 0.0536\n",
            "Epoch 106/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0301 - rmse: 0.1090 - val_loss: 0.0226 - val_rmse: 0.0678\n",
            "Epoch 107/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0316 - rmse: 0.1167 - val_loss: 0.0200 - val_rmse: 0.0472\n",
            "Epoch 108/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0303 - rmse: 0.1117 - val_loss: 0.0204 - val_rmse: 0.0537\n",
            "Epoch 109/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0286 - rmse: 0.1053 - val_loss: 0.0209 - val_rmse: 0.0590\n",
            "Epoch 110/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0290 - rmse: 0.1077 - val_loss: 0.0201 - val_rmse: 0.0537\n",
            "Epoch 111/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0297 - rmse: 0.1121 - val_loss: 0.0200 - val_rmse: 0.0551\n",
            "Epoch 112/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0275 - rmse: 0.1026 - val_loss: 0.0199 - val_rmse: 0.0556\n",
            "Epoch 113/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0299 - rmse: 0.1145 - val_loss: 0.0190 - val_rmse: 0.0489\n",
            "Epoch 114/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0265 - rmse: 0.1000 - val_loss: 0.0187 - val_rmse: 0.0475\n",
            "Epoch 115/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0274 - rmse: 0.1047 - val_loss: 0.0199 - val_rmse: 0.0602\n",
            "Epoch 116/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0278 - rmse: 0.1076 - val_loss: 0.0194 - val_rmse: 0.0580\n",
            "Epoch 117/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0276 - rmse: 0.1079 - val_loss: 0.0181 - val_rmse: 0.0466\n",
            "Epoch 118/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0279 - rmse: 0.1100 - val_loss: 0.0183 - val_rmse: 0.0511\n",
            "Epoch 119/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0259 - rmse: 0.1014 - val_loss: 0.0182 - val_rmse: 0.0526\n",
            "Epoch 120/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0263 - rmse: 0.1041 - val_loss: 0.0183 - val_rmse: 0.0544\n",
            "Epoch 121/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0261 - rmse: 0.1039 - val_loss: 0.0182 - val_rmse: 0.0559\n",
            "Epoch 122/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0271 - rmse: 0.1095 - val_loss: 0.0179 - val_rmse: 0.0538\n",
            "Epoch 123/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0282 - rmse: 0.1146 - val_loss: 0.0177 - val_rmse: 0.0537\n",
            "Epoch 124/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0251 - rmse: 0.1017 - val_loss: 0.0172 - val_rmse: 0.0507\n",
            "Epoch 125/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0264 - rmse: 0.1087 - val_loss: 0.0182 - val_rmse: 0.0613\n",
            "Epoch 126/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0265 - rmse: 0.1097 - val_loss: 0.0170 - val_rmse: 0.0514\n",
            "Epoch 127/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0256 - rmse: 0.1062 - val_loss: 0.0166 - val_rmse: 0.0501\n",
            "Epoch 128/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0244 - rmse: 0.1012 - val_loss: 0.0161 - val_rmse: 0.0462\n",
            "Epoch 129/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0254 - rmse: 0.1068 - val_loss: 0.0168 - val_rmse: 0.0544\n",
            "Epoch 130/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0265 - rmse: 0.1128 - val_loss: 0.0158 - val_rmse: 0.0462\n",
            "Epoch 131/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0247 - rmse: 0.1051 - val_loss: 0.0166 - val_rmse: 0.0554\n",
            "Epoch 132/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0241 - rmse: 0.1027 - val_loss: 0.0163 - val_rmse: 0.0541\n",
            "Epoch 133/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0246 - rmse: 0.1059 - val_loss: 0.0154 - val_rmse: 0.0461\n",
            "Epoch 134/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0238 - rmse: 0.1032 - val_loss: 0.0153 - val_rmse: 0.0469\n",
            "Epoch 135/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0229 - rmse: 0.0991 - val_loss: 0.0156 - val_rmse: 0.0518\n",
            "Epoch 136/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0225 - rmse: 0.0978 - val_loss: 0.0161 - val_rmse: 0.0575\n",
            "Epoch 137/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0238 - rmse: 0.1050 - val_loss: 0.0150 - val_rmse: 0.0487\n",
            "Epoch 138/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0240 - rmse: 0.1066 - val_loss: 0.0156 - val_rmse: 0.0553\n",
            "Epoch 139/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0226 - rmse: 0.1008 - val_loss: 0.0146 - val_rmse: 0.0468\n",
            "Epoch 140/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0216 - rmse: 0.0960 - val_loss: 0.0153 - val_rmse: 0.0554\n",
            "Epoch 141/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0209 - rmse: 0.0928 - val_loss: 0.0148 - val_rmse: 0.0522\n",
            "Epoch 142/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0225 - rmse: 0.1020 - val_loss: 0.0142 - val_rmse: 0.0466\n",
            "Epoch 143/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0215 - rmse: 0.0972 - val_loss: 0.0147 - val_rmse: 0.0532\n",
            "Epoch 144/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0216 - rmse: 0.0990 - val_loss: 0.0142 - val_rmse: 0.0489\n",
            "Epoch 145/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0216 - rmse: 0.0992 - val_loss: 0.0138 - val_rmse: 0.0472\n",
            "Epoch 146/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0215 - rmse: 0.0998 - val_loss: 0.0139 - val_rmse: 0.0494\n",
            "Epoch 147/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0214 - rmse: 0.0997 - val_loss: 0.0137 - val_rmse: 0.0484\n",
            "Epoch 148/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0205 - rmse: 0.0959 - val_loss: 0.0132 - val_rmse: 0.0444\n",
            "Epoch 149/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0218 - rmse: 0.1027 - val_loss: 0.0138 - val_rmse: 0.0517\n",
            "Epoch 150/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0201 - rmse: 0.0949 - val_loss: 0.0132 - val_rmse: 0.0468\n",
            "Epoch 151/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0197 - rmse: 0.0934 - val_loss: 0.0131 - val_rmse: 0.0473\n",
            "Epoch 152/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0206 - rmse: 0.0986 - val_loss: 0.0135 - val_rmse: 0.0522\n",
            "Epoch 153/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0202 - rmse: 0.0969 - val_loss: 0.0136 - val_rmse: 0.0549\n",
            "Epoch 154/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0221 - rmse: 0.1070 - val_loss: 0.0127 - val_rmse: 0.0465\n",
            "Epoch 155/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0208 - rmse: 0.1015 - val_loss: 0.0128 - val_rmse: 0.0490\n",
            "Epoch 156/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0188 - rmse: 0.0918 - val_loss: 0.0124 - val_rmse: 0.0458\n",
            "Epoch 157/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0196 - rmse: 0.0965 - val_loss: 0.0125 - val_rmse: 0.0487\n",
            "Epoch 158/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0203 - rmse: 0.1007 - val_loss: 0.0121 - val_rmse: 0.0445\n",
            "Epoch 159/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0202 - rmse: 0.1008 - val_loss: 0.0123 - val_rmse: 0.0484\n",
            "Epoch 160/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0208 - rmse: 0.1042 - val_loss: 0.0122 - val_rmse: 0.0484\n",
            "Epoch 161/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0191 - rmse: 0.0959 - val_loss: 0.0120 - val_rmse: 0.0473\n",
            "Epoch 162/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0202 - rmse: 0.1018 - val_loss: 0.0134 - val_rmse: 0.0607\n",
            "Epoch 163/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0207 - rmse: 0.1050 - val_loss: 0.0115 - val_rmse: 0.0432\n",
            "Epoch 164/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0186 - rmse: 0.0948 - val_loss: 0.0124 - val_rmse: 0.0545\n",
            "Epoch 165/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0178 - rmse: 0.0912 - val_loss: 0.0116 - val_rmse: 0.0472\n",
            "Epoch 166/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0171 - rmse: 0.0880 - val_loss: 0.0112 - val_rmse: 0.0441\n",
            "Epoch 167/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0192 - rmse: 0.0999 - val_loss: 0.0119 - val_rmse: 0.0518\n",
            "Epoch 168/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0193 - rmse: 0.1007 - val_loss: 0.0112 - val_rmse: 0.0455\n",
            "Epoch 169/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0177 - rmse: 0.0928 - val_loss: 0.0115 - val_rmse: 0.0494\n",
            "Epoch 170/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0180 - rmse: 0.0946 - val_loss: 0.0118 - val_rmse: 0.0542\n",
            "Epoch 171/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0181 - rmse: 0.0958 - val_loss: 0.0106 - val_rmse: 0.0414\n",
            "Epoch 172/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0190 - rmse: 0.1006 - val_loss: 0.0108 - val_rmse: 0.0456\n",
            "Epoch 173/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0182 - rmse: 0.0976 - val_loss: 0.0107 - val_rmse: 0.0454\n",
            "Epoch 174/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0167 - rmse: 0.0895 - val_loss: 0.0103 - val_rmse: 0.0412\n",
            "Epoch 175/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0177 - rmse: 0.0953 - val_loss: 0.0107 - val_rmse: 0.0470\n",
            "Epoch 176/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0177 - rmse: 0.0958 - val_loss: 0.0105 - val_rmse: 0.0454\n",
            "Epoch 177/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0175 - rmse: 0.0952 - val_loss: 0.0109 - val_rmse: 0.0503\n",
            "Epoch 178/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0167 - rmse: 0.0918 - val_loss: 0.0109 - val_rmse: 0.0518\n",
            "Epoch 179/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0174 - rmse: 0.0955 - val_loss: 0.0096 - val_rmse: 0.0370\n",
            "Epoch 180/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0179 - rmse: 0.0979 - val_loss: 0.0114 - val_rmse: 0.0578\n",
            "Epoch 181/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0163 - rmse: 0.0908 - val_loss: 0.0096 - val_rmse: 0.0401\n",
            "Epoch 182/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0162 - rmse: 0.0906 - val_loss: 0.0100 - val_rmse: 0.0452\n",
            "Epoch 183/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0157 - rmse: 0.0882 - val_loss: 0.0095 - val_rmse: 0.0400\n",
            "Epoch 184/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0176 - rmse: 0.0983 - val_loss: 0.0095 - val_rmse: 0.0401\n",
            "Epoch 185/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0166 - rmse: 0.0934 - val_loss: 0.0098 - val_rmse: 0.0448\n",
            "Epoch 186/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0162 - rmse: 0.0918 - val_loss: 0.0108 - val_rmse: 0.0557\n",
            "Epoch 187/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0177 - rmse: 0.1000 - val_loss: 0.0099 - val_rmse: 0.0474\n",
            "Epoch 188/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0160 - rmse: 0.0913 - val_loss: 0.0090 - val_rmse: 0.0379\n",
            "Epoch 189/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0152 - rmse: 0.0873 - val_loss: 0.0094 - val_rmse: 0.0433\n",
            "Epoch 190/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0144 - rmse: 0.0831 - val_loss: 0.0091 - val_rmse: 0.0409\n",
            "Epoch 191/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0162 - rmse: 0.0936 - val_loss: 0.0094 - val_rmse: 0.0455\n",
            "Epoch 192/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0162 - rmse: 0.0944 - val_loss: 0.0105 - val_rmse: 0.0571\n",
            "Epoch 193/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0165 - rmse: 0.0960 - val_loss: 0.0099 - val_rmse: 0.0510\n",
            "Epoch 194/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0165 - rmse: 0.0958 - val_loss: 0.0097 - val_rmse: 0.0511\n",
            "Epoch 195/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0156 - rmse: 0.0920 - val_loss: 0.0087 - val_rmse: 0.0398\n",
            "Epoch 196/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0146 - rmse: 0.0864 - val_loss: 0.0096 - val_rmse: 0.0497\n",
            "Epoch 197/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0152 - rmse: 0.0902 - val_loss: 0.0089 - val_rmse: 0.0438\n",
            "Epoch 198/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0144 - rmse: 0.0858 - val_loss: 0.0094 - val_rmse: 0.0495\n",
            "Epoch 199/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0150 - rmse: 0.0901 - val_loss: 0.0084 - val_rmse: 0.0381\n",
            "Epoch 200/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0151 - rmse: 0.0905 - val_loss: 0.0100 - val_rmse: 0.0570\n",
            "Epoch 201/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0148 - rmse: 0.0890 - val_loss: 0.0085 - val_rmse: 0.0412\n",
            "Epoch 202/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0153 - rmse: 0.0924 - val_loss: 0.0081 - val_rmse: 0.0370\n",
            "Epoch 203/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0156 - rmse: 0.0936 - val_loss: 0.0090 - val_rmse: 0.0479\n",
            "Epoch 204/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0146 - rmse: 0.0891 - val_loss: 0.0082 - val_rmse: 0.0390\n",
            "Epoch 205/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0140 - rmse: 0.0858 - val_loss: 0.0083 - val_rmse: 0.0414\n",
            "Epoch 206/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0145 - rmse: 0.0890 - val_loss: 0.0083 - val_rmse: 0.0424\n",
            "Epoch 207/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0155 - rmse: 0.0944 - val_loss: 0.0086 - val_rmse: 0.0463\n",
            "Epoch 208/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0152 - rmse: 0.0936 - val_loss: 0.0082 - val_rmse: 0.0419\n",
            "Epoch 209/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0154 - rmse: 0.0944 - val_loss: 0.0083 - val_rmse: 0.0431\n",
            "Epoch 210/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0152 - rmse: 0.0937 - val_loss: 0.0091 - val_rmse: 0.0521\n",
            "Epoch 211/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0140 - rmse: 0.0876 - val_loss: 0.0078 - val_rmse: 0.0387\n",
            "Epoch 212/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0143 - rmse: 0.0891 - val_loss: 0.0087 - val_rmse: 0.0495\n",
            "Epoch 213/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0142 - rmse: 0.0889 - val_loss: 0.0075 - val_rmse: 0.0348\n",
            "Epoch 214/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0146 - rmse: 0.0916 - val_loss: 0.0078 - val_rmse: 0.0408\n",
            "Epoch 215/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0152 - rmse: 0.0951 - val_loss: 0.0085 - val_rmse: 0.0481\n",
            "Epoch 216/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0140 - rmse: 0.0884 - val_loss: 0.0079 - val_rmse: 0.0431\n",
            "Epoch 217/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0136 - rmse: 0.0864 - val_loss: 0.0079 - val_rmse: 0.0427\n",
            "Epoch 218/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0167 - rmse: 0.1028 - val_loss: 0.0074 - val_rmse: 0.0368\n",
            "Epoch 219/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0149 - rmse: 0.0938 - val_loss: 0.0081 - val_rmse: 0.0460\n",
            "Epoch 220/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0146 - rmse: 0.0926 - val_loss: 0.0079 - val_rmse: 0.0440\n",
            "Epoch 221/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0148 - rmse: 0.0939 - val_loss: 0.0071 - val_rmse: 0.0332\n",
            "Epoch 222/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0139 - rmse: 0.0891 - val_loss: 0.0074 - val_rmse: 0.0390\n",
            "Epoch 223/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0134 - rmse: 0.0870 - val_loss: 0.0074 - val_rmse: 0.0399\n",
            "Epoch 224/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0124 - rmse: 0.0809 - val_loss: 0.0083 - val_rmse: 0.0504\n",
            "Epoch 225/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0143 - rmse: 0.0923 - val_loss: 0.0074 - val_rmse: 0.0407\n",
            "Epoch 226/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0134 - rmse: 0.0873 - val_loss: 0.0073 - val_rmse: 0.0396\n",
            "Epoch 227/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0129 - rmse: 0.0849 - val_loss: 0.0076 - val_rmse: 0.0434\n",
            "Epoch 228/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0128 - rmse: 0.0843 - val_loss: 0.0076 - val_rmse: 0.0438\n",
            "Epoch 229/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0149 - rmse: 0.0958 - val_loss: 0.0072 - val_rmse: 0.0405\n",
            "Epoch 230/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0140 - rmse: 0.0916 - val_loss: 0.0074 - val_rmse: 0.0420\n",
            "Epoch 231/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0140 - rmse: 0.0912 - val_loss: 0.0075 - val_rmse: 0.0444\n",
            "Epoch 232/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0146 - rmse: 0.0948 - val_loss: 0.0071 - val_rmse: 0.0394\n",
            "Epoch 233/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0125 - rmse: 0.0834 - val_loss: 0.0067 - val_rmse: 0.0339\n",
            "Epoch 234/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0147 - rmse: 0.0952 - val_loss: 0.0079 - val_rmse: 0.0488\n",
            "Epoch 235/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0148 - rmse: 0.0964 - val_loss: 0.0074 - val_rmse: 0.0440\n",
            "Epoch 236/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0137 - rmse: 0.0909 - val_loss: 0.0070 - val_rmse: 0.0391\n",
            "Epoch 237/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0136 - rmse: 0.0903 - val_loss: 0.0070 - val_rmse: 0.0389\n",
            "Epoch 238/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0136 - rmse: 0.0904 - val_loss: 0.0070 - val_rmse: 0.0401\n",
            "Epoch 239/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0143 - rmse: 0.0943 - val_loss: 0.0068 - val_rmse: 0.0379\n",
            "Epoch 240/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0125 - rmse: 0.0844 - val_loss: 0.0065 - val_rmse: 0.0338\n",
            "Epoch 241/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0135 - rmse: 0.0902 - val_loss: 0.0064 - val_rmse: 0.0341\n",
            "Epoch 242/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0142 - rmse: 0.0941 - val_loss: 0.0068 - val_rmse: 0.0392\n",
            "Epoch 243/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0130 - rmse: 0.0882 - val_loss: 0.0067 - val_rmse: 0.0381\n",
            "Epoch 244/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0132 - rmse: 0.0894 - val_loss: 0.0068 - val_rmse: 0.0392\n",
            "Epoch 245/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0123 - rmse: 0.0844 - val_loss: 0.0063 - val_rmse: 0.0332\n",
            "Epoch 246/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0117 - rmse: 0.0807 - val_loss: 0.0070 - val_rmse: 0.0438\n",
            "Epoch 247/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0127 - rmse: 0.0874 - val_loss: 0.0074 - val_rmse: 0.0478\n",
            "Epoch 248/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0121 - rmse: 0.0836 - val_loss: 0.0063 - val_rmse: 0.0350\n",
            "Epoch 249/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0125 - rmse: 0.0860 - val_loss: 0.0065 - val_rmse: 0.0380\n",
            "Epoch 250/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0122 - rmse: 0.0849 - val_loss: 0.0061 - val_rmse: 0.0319\n",
            "Epoch 251/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0137 - rmse: 0.0929 - val_loss: 0.0082 - val_rmse: 0.0566\n",
            "Epoch 252/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0124 - rmse: 0.0860 - val_loss: 0.0068 - val_rmse: 0.0433\n",
            "Epoch 253/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0125 - rmse: 0.0867 - val_loss: 0.0074 - val_rmse: 0.0485\n",
            "Epoch 254/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0130 - rmse: 0.0894 - val_loss: 0.0067 - val_rmse: 0.0421\n",
            "Epoch 255/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0133 - rmse: 0.0913 - val_loss: 0.0062 - val_rmse: 0.0359\n",
            "Epoch 256/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0125 - rmse: 0.0871 - val_loss: 0.0061 - val_rmse: 0.0342\n",
            "Epoch 257/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0130 - rmse: 0.0900 - val_loss: 0.0062 - val_rmse: 0.0361\n",
            "Epoch 258/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0123 - rmse: 0.0860 - val_loss: 0.0065 - val_rmse: 0.0411\n",
            "Epoch 259/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0126 - rmse: 0.0882 - val_loss: 0.0063 - val_rmse: 0.0386\n",
            "Epoch 260/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0124 - rmse: 0.0870 - val_loss: 0.0072 - val_rmse: 0.0488\n",
            "Epoch 261/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0118 - rmse: 0.0833 - val_loss: 0.0066 - val_rmse: 0.0434\n",
            "Epoch 262/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0129 - rmse: 0.0899 - val_loss: 0.0068 - val_rmse: 0.0453\n",
            "Epoch 263/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0109 - rmse: 0.0779 - val_loss: 0.0060 - val_rmse: 0.0349\n",
            "Epoch 264/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0131 - rmse: 0.0910 - val_loss: 0.0059 - val_rmse: 0.0343\n",
            "Epoch 265/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0114 - rmse: 0.0815 - val_loss: 0.0058 - val_rmse: 0.0333\n",
            "Epoch 266/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0113 - rmse: 0.0812 - val_loss: 0.0062 - val_rmse: 0.0386\n",
            "Epoch 267/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0120 - rmse: 0.0856 - val_loss: 0.0066 - val_rmse: 0.0440\n",
            "Epoch 268/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0124 - rmse: 0.0882 - val_loss: 0.0058 - val_rmse: 0.0334\n",
            "Epoch 269/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0125 - rmse: 0.0885 - val_loss: 0.0057 - val_rmse: 0.0318\n",
            "Epoch 270/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0116 - rmse: 0.0836 - val_loss: 0.0055 - val_rmse: 0.0295\n",
            "Epoch 271/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0122 - rmse: 0.0868 - val_loss: 0.0060 - val_rmse: 0.0370\n",
            "Epoch 272/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0138 - rmse: 0.0955 - val_loss: 0.0057 - val_rmse: 0.0332\n",
            "Epoch 273/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0136 - rmse: 0.0944 - val_loss: 0.0063 - val_rmse: 0.0414\n",
            "Epoch 274/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0115 - rmse: 0.0833 - val_loss: 0.0058 - val_rmse: 0.0353\n",
            "Epoch 275/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0133 - rmse: 0.0933 - val_loss: 0.0063 - val_rmse: 0.0424\n",
            "Epoch 276/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0116 - rmse: 0.0843 - val_loss: 0.0066 - val_rmse: 0.0455\n",
            "Epoch 277/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0125 - rmse: 0.0890 - val_loss: 0.0065 - val_rmse: 0.0453\n",
            "Epoch 278/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0119 - rmse: 0.0858 - val_loss: 0.0056 - val_rmse: 0.0325\n",
            "Epoch 279/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0116 - rmse: 0.0843 - val_loss: 0.0058 - val_rmse: 0.0363\n",
            "Epoch 280/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0116 - rmse: 0.0844 - val_loss: 0.0060 - val_rmse: 0.0401\n",
            "Epoch 281/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0111 - rmse: 0.0816 - val_loss: 0.0059 - val_rmse: 0.0387\n",
            "Epoch 282/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0129 - rmse: 0.0920 - val_loss: 0.0060 - val_rmse: 0.0396\n",
            "Epoch 283/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0120 - rmse: 0.0869 - val_loss: 0.0066 - val_rmse: 0.0472\n",
            "Epoch 284/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0123 - rmse: 0.0889 - val_loss: 0.0054 - val_rmse: 0.0318\n",
            "Epoch 285/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0116 - rmse: 0.0845 - val_loss: 0.0057 - val_rmse: 0.0361\n",
            "Epoch 286/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0112 - rmse: 0.0821 - val_loss: 0.0060 - val_rmse: 0.0400\n",
            "Epoch 287/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0115 - rmse: 0.0841 - val_loss: 0.0057 - val_rmse: 0.0364\n",
            "Epoch 288/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0126 - rmse: 0.0909 - val_loss: 0.0056 - val_rmse: 0.0358\n",
            "Epoch 289/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0135 - rmse: 0.0956 - val_loss: 0.0068 - val_rmse: 0.0504\n",
            "Epoch 290/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0123 - rmse: 0.0893 - val_loss: 0.0059 - val_rmse: 0.0393\n",
            "Epoch 291/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0121 - rmse: 0.0879 - val_loss: 0.0064 - val_rmse: 0.0449\n",
            "Epoch 292/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0131 - rmse: 0.0932 - val_loss: 0.0072 - val_rmse: 0.0536\n",
            "Epoch 293/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0121 - rmse: 0.0878 - val_loss: 0.0055 - val_rmse: 0.0351\n",
            "Epoch 294/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0122 - rmse: 0.0885 - val_loss: 0.0053 - val_rmse: 0.0319\n",
            "Epoch 295/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0114 - rmse: 0.0842 - val_loss: 0.0058 - val_rmse: 0.0388\n",
            "Epoch 296/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0129 - rmse: 0.0927 - val_loss: 0.0064 - val_rmse: 0.0462\n",
            "Epoch 297/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0125 - rmse: 0.0907 - val_loss: 0.0065 - val_rmse: 0.0470\n",
            "Epoch 298/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0131 - rmse: 0.0939 - val_loss: 0.0061 - val_rmse: 0.0423\n",
            "Epoch 299/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0113 - rmse: 0.0838 - val_loss: 0.0055 - val_rmse: 0.0346\n",
            "Epoch 300/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0135 - rmse: 0.0957 - val_loss: 0.0061 - val_rmse: 0.0435\n",
            "Epoch 301/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0112 - rmse: 0.0836 - val_loss: 0.0053 - val_rmse: 0.0334\n",
            "Epoch 302/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0117 - rmse: 0.0866 - val_loss: 0.0053 - val_rmse: 0.0339\n",
            "Epoch 303/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0123 - rmse: 0.0900 - val_loss: 0.0063 - val_rmse: 0.0467\n",
            "Epoch 304/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0121 - rmse: 0.0889 - val_loss: 0.0058 - val_rmse: 0.0409\n",
            "Epoch 305/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0121 - rmse: 0.0892 - val_loss: 0.0054 - val_rmse: 0.0347\n",
            "Epoch 306/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0114 - rmse: 0.0852 - val_loss: 0.0058 - val_rmse: 0.0407\n",
            "Epoch 307/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0125 - rmse: 0.0915 - val_loss: 0.0050 - val_rmse: 0.0296\n",
            "Epoch 308/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0124 - rmse: 0.0910 - val_loss: 0.0058 - val_rmse: 0.0406\n",
            "Epoch 309/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0135 - rmse: 0.0966 - val_loss: 0.0055 - val_rmse: 0.0372\n",
            "Epoch 310/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0107 - rmse: 0.0814 - val_loss: 0.0054 - val_rmse: 0.0367\n",
            "Epoch 311/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0113 - rmse: 0.0848 - val_loss: 0.0062 - val_rmse: 0.0463\n",
            "Epoch 312/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0119 - rmse: 0.0888 - val_loss: 0.0062 - val_rmse: 0.0455\n",
            "Epoch 313/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0121 - rmse: 0.0898 - val_loss: 0.0056 - val_rmse: 0.0395\n",
            "Epoch 314/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0113 - rmse: 0.0855 - val_loss: 0.0053 - val_rmse: 0.0350\n",
            "Epoch 315/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0113 - rmse: 0.0856 - val_loss: 0.0052 - val_rmse: 0.0341\n",
            "Epoch 316/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0110 - rmse: 0.0838 - val_loss: 0.0059 - val_rmse: 0.0439\n",
            "Epoch 317/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0118 - rmse: 0.0884 - val_loss: 0.0053 - val_rmse: 0.0363\n",
            "Epoch 318/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0120 - rmse: 0.0896 - val_loss: 0.0055 - val_rmse: 0.0391\n",
            "Epoch 319/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0117 - rmse: 0.0878 - val_loss: 0.0060 - val_rmse: 0.0460\n",
            "Epoch 320/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0107 - rmse: 0.0820 - val_loss: 0.0051 - val_rmse: 0.0343\n",
            "Epoch 321/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0114 - rmse: 0.0861 - val_loss: 0.0061 - val_rmse: 0.0475\n",
            "Epoch 322/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0125 - rmse: 0.0923 - val_loss: 0.0057 - val_rmse: 0.0419\n",
            "Epoch 323/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0118 - rmse: 0.0886 - val_loss: 0.0060 - val_rmse: 0.0452\n",
            "Epoch 324/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0116 - rmse: 0.0876 - val_loss: 0.0061 - val_rmse: 0.0471\n",
            "Epoch 325/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0119 - rmse: 0.0894 - val_loss: 0.0051 - val_rmse: 0.0347\n",
            "Epoch 326/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0113 - rmse: 0.0856 - val_loss: 0.0057 - val_rmse: 0.0426\n",
            "Epoch 327/500\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0113 - rmse: 0.0859 - val_loss: 0.0050 - val_rmse: 0.0330\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate"
      ],
      "metadata": {
        "id": "PT7MAPfQULat"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Eval (scaled y):\", model.evaluate(test_ds, verbose=0))\n",
        "\n",
        "# Dự đoán trên TEST (đang ở thang đã scale)\n",
        "y_pred_s = model.predict(X_test_s, batch_size=BATCH_SIZE)\n",
        "# Đảo về thang đo thật\n",
        "y_pred = scaler_y.inverse_transform(y_pred_s)\n",
        "\n",
        "# R² và RMSE trên thang đo thật\n",
        "r2 = r2_score(y_test, y_pred)  # CHÚ Ý: (y_true, y_pred)\n",
        "rmse = np.sqrt(np.mean((y_pred - y_test) ** 2))\n",
        "mae  = np.mean(np.abs(y_pred - y_test))\n",
        "\n",
        "print(f\"Test R2 (original scale):  {r2:.4f}\")\n",
        "print(f\"Test RMSE (original scale): {rmse:.4f}\")\n",
        "print(f\"Test MAE  (original scale): {mae:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-6qZhY6H5wEM",
        "outputId": "9adb494f-50bb-4e42-8f94-5a4699d0d7e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval (scaled y): [0.005159873515367508, 0.03217161446809769]\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
            "Test R2 (original scale):  0.9989\n",
            "Test RMSE (original scale): 4056.6466\n",
            "Test MAE  (original scale): 3145.0366\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code using Pytorch"
      ],
      "metadata": {
        "id": "1qRbGP330LPa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import libraries"
      ],
      "metadata": {
        "id": "zseXavyS0WqA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import torch.optim as optim\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import r2_score"
      ],
      "metadata": {
        "id": "I9uzFu8S0YYJ"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Data"
      ],
      "metadata": {
        "id": "7bQN1itL0b2w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "df = pd.read_csv('/content/drive/MyDrive/train.csv')\n",
        "\n",
        "# Drop the first column (as in your TF code) and split X/y\n",
        "data_np = df.drop(df.columns[0], axis=1).to_numpy()\n",
        "X = data_np[:, :-1].astype(np.float32)\n",
        "y = data_np[:, -1].astype(np.float32).reshape(-1, 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uu87ExRT0e2w",
        "outputId": "769f91ed-5e93-4494-e4be-02286421e901"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare the data"
      ],
      "metadata": {
        "id": "PZAtC8zy0uR9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TRAIN_RATIO, VAL_RATIO = 0.8, 0.1\n",
        "BATCH_SIZE, SEED = 64, 123\n",
        "LR = 1e-3\n",
        "L2_LAMBDA = 1e-3\n",
        "DROP1, DROP2 = 0.1, 0.05\n",
        "EPOCHS = 500\n",
        "PATIENCE = 20\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EvjbiaLI0wCC",
        "outputId": "d29590e3-5834-473b-e4e2-6d6daa1d571a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n = len(X)\n",
        "idx = np.random.RandomState(42).permutation(n)  # similar to tf.random.shuffle(seed=42)\n",
        "\n",
        "n_train = int(n * TRAIN_RATIO)\n",
        "n_val = int(n * VAL_RATIO)\n",
        "n_test = n - n_train - n_val\n",
        "\n",
        "idx_train = idx[:n_train]\n",
        "idx_val   = idx[n_train:n_train + n_val]\n",
        "idx_test  = idx[n_train + n_val:]\n",
        "\n",
        "X_train, y_train = X[idx_train], y[idx_train]\n",
        "X_val,   y_val   = X[idx_val],   y[idx_val]\n",
        "X_test,  y_test  = X[idx_test],  y[idx_test]\n",
        "\n",
        "scaler_X = StandardScaler()\n",
        "X_train_s = scaler_X.fit_transform(X_train)\n",
        "X_val_s   = scaler_X.transform(X_val)\n",
        "X_test_s  = scaler_X.transform(X_test)\n",
        "\n",
        "scaler_y = StandardScaler()\n",
        "y_train_s = scaler_y.fit_transform(y_train)\n",
        "y_val_s   = scaler_y.transform(y_val)\n",
        "y_test_s  = scaler_y.transform(y_test)\n",
        "\n",
        "def to_tensor(x): return torch.tensor(x, dtype=torch.float32)\n",
        "train_ds = TensorDataset(to_tensor(X_train_s), to_tensor(y_train_s))\n",
        "val_ds   = TensorDataset(to_tensor(X_val_s),   to_tensor(y_val_s))\n",
        "test_ds  = TensorDataset(to_tensor(X_test_s),  to_tensor(y_test_s))\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,  drop_last=False)\n",
        "val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False, drop_last=False)\n",
        "test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False, drop_last=False)"
      ],
      "metadata": {
        "id": "Zq3ceu2r0xNX"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model"
      ],
      "metadata": {
        "id": "tqD0Hetk1Wg5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "in_dim = X_train_s.shape[1]\n",
        "\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(in_dim, 64),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(DROP1),\n",
        "    nn.Linear(64, 32),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(DROP2),\n",
        "    nn.Linear(32, 1),\n",
        ").to(device)\n",
        "\n",
        "# Loss and Optimizer\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=LR, weight_decay=L2_LAMBDA)"
      ],
      "metadata": {
        "id": "3OYam_so1YS3"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loss and Optimizer"
      ],
      "metadata": {
        "id": "6AjFM2551kBo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=LR, weight_decay=L2_LAMBDA)"
      ],
      "metadata": {
        "id": "r6PSopti1lv8"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Early Stopping"
      ],
      "metadata": {
        "id": "lMCyqVt01oUj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EarlyStopping:\n",
        "    def __init__(self, patience=10, mode=\"min\", restore_best=True):\n",
        "        self.patience = patience\n",
        "        self.mode = mode\n",
        "        self.restore_best = restore_best\n",
        "        self.best_score = None\n",
        "        self.best_state = None\n",
        "        self.counter = 0\n",
        "\n",
        "    def step(self, current_score, model):\n",
        "        # For \"min\" mode, improvement means current_score < best_score\n",
        "        if self.best_score is None or \\\n",
        "           (self.mode == \"min\" and current_score < self.best_score - 1e-12) or \\\n",
        "           (self.mode == \"max\" and current_score > self.best_score + 1e-12):\n",
        "            self.best_score = current_score\n",
        "            self.counter = 0\n",
        "            if self.restore_best:\n",
        "                # Keep a deep copy of the weights\n",
        "                self.best_state = {k: v.detach().clone() for k, v in model.state_dict().items()}\n",
        "            return False  # do not stop\n",
        "        else:\n",
        "            self.counter += 1\n",
        "            return self.counter > self.patience  # stop if patience exceeded\n",
        "\n",
        "    def restore(self, model):\n",
        "        if self.restore_best and self.best_state is not None:\n",
        "            model.load_state_dict(self.best_state)\n",
        "\n",
        "early_stopper = EarlyStopping(patience=PATIENCE, mode=\"min\", restore_best=True)"
      ],
      "metadata": {
        "id": "97xIU8JS1pbl"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "0UdQg7pu1uLv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_epoch(loader, model, optimizer=None):\n",
        "    is_train = optimizer is not None\n",
        "    model.train(is_train)\n",
        "\n",
        "    running_loss = 0.0\n",
        "    n_samples = 0\n",
        "\n",
        "    for xb, yb in loader:\n",
        "        xb = xb.to(device)\n",
        "        yb = yb.to(device)\n",
        "\n",
        "        if is_train:\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        preds = model(xb)\n",
        "        loss = criterion(preds, yb)\n",
        "\n",
        "        if is_train:\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        batch_size = xb.size(0)\n",
        "        running_loss += loss.item() * batch_size\n",
        "        n_samples += batch_size\n",
        "\n",
        "    avg_loss = running_loss / max(1, n_samples)\n",
        "    rmse = np.sqrt(avg_loss)  # since loss is MSE on scaled targets\n",
        "    return avg_loss, rmse\n",
        "\n",
        "best_val = np.inf\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    train_loss, train_rmse = run_epoch(train_loader, model, optimizer)\n",
        "    with torch.no_grad():\n",
        "        val_loss, val_rmse = run_epoch(val_loader, model, optimizer=None)\n",
        "\n",
        "    if epoch % 1 == 0:\n",
        "        print(f\"Epoch {epoch:03d} | \"\n",
        "              f\"train_loss={train_loss:.6f} train_rmse={train_rmse:.6f} | \"\n",
        "              f\"val_loss={val_loss:.6f} val_rmse={val_rmse:.6f}\")\n",
        "\n",
        "    # Early stopping check on val_loss\n",
        "    stop = early_stopper.step(val_loss, model)\n",
        "    if stop:\n",
        "        print(f\"Early stopping at epoch {epoch}. Best val_loss: {early_stopper.best_score:.6f}\")\n",
        "        break\n",
        "\n",
        "# Restore best weights\n",
        "early_stopper.restore(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "POMSj-WO1xlU",
        "outputId": "39190c0e-ba16-49dd-895f-1845108e264c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train_loss=0.969683 train_rmse=0.984725 | val_loss=0.825438 val_rmse=0.908536\n",
            "Epoch 002 | train_loss=0.780927 train_rmse=0.883701 | val_loss=0.622591 val_rmse=0.789045\n",
            "Epoch 003 | train_loss=0.541797 train_rmse=0.736069 | val_loss=0.341676 val_rmse=0.584531\n",
            "Epoch 004 | train_loss=0.250711 train_rmse=0.500710 | val_loss=0.094362 val_rmse=0.307185\n",
            "Epoch 005 | train_loss=0.097847 train_rmse=0.312805 | val_loss=0.033861 val_rmse=0.184013\n",
            "Epoch 006 | train_loss=0.070889 train_rmse=0.266250 | val_loss=0.026022 val_rmse=0.161314\n",
            "Epoch 007 | train_loss=0.055770 train_rmse=0.236156 | val_loss=0.019219 val_rmse=0.138633\n",
            "Epoch 008 | train_loss=0.044647 train_rmse=0.211298 | val_loss=0.016056 val_rmse=0.126711\n",
            "Epoch 009 | train_loss=0.045503 train_rmse=0.213314 | val_loss=0.013918 val_rmse=0.117976\n",
            "Epoch 010 | train_loss=0.043569 train_rmse=0.208732 | val_loss=0.011813 val_rmse=0.108688\n",
            "Epoch 011 | train_loss=0.035594 train_rmse=0.188663 | val_loss=0.012568 val_rmse=0.112105\n",
            "Epoch 012 | train_loss=0.039436 train_rmse=0.198585 | val_loss=0.011047 val_rmse=0.105106\n",
            "Epoch 013 | train_loss=0.033260 train_rmse=0.182374 | val_loss=0.010737 val_rmse=0.103619\n",
            "Epoch 014 | train_loss=0.033456 train_rmse=0.182909 | val_loss=0.009985 val_rmse=0.099927\n",
            "Epoch 015 | train_loss=0.033961 train_rmse=0.184286 | val_loss=0.009377 val_rmse=0.096833\n",
            "Epoch 016 | train_loss=0.032403 train_rmse=0.180008 | val_loss=0.008990 val_rmse=0.094816\n",
            "Epoch 017 | train_loss=0.029021 train_rmse=0.170357 | val_loss=0.008907 val_rmse=0.094377\n",
            "Epoch 018 | train_loss=0.029078 train_rmse=0.170523 | val_loss=0.010091 val_rmse=0.100453\n",
            "Epoch 019 | train_loss=0.030769 train_rmse=0.175411 | val_loss=0.008443 val_rmse=0.091888\n",
            "Epoch 020 | train_loss=0.032383 train_rmse=0.179954 | val_loss=0.007620 val_rmse=0.087295\n",
            "Epoch 021 | train_loss=0.024193 train_rmse=0.155540 | val_loss=0.008375 val_rmse=0.091517\n",
            "Epoch 022 | train_loss=0.027388 train_rmse=0.165494 | val_loss=0.007792 val_rmse=0.088275\n",
            "Epoch 023 | train_loss=0.029707 train_rmse=0.172359 | val_loss=0.006783 val_rmse=0.082361\n",
            "Epoch 024 | train_loss=0.026824 train_rmse=0.163781 | val_loss=0.006855 val_rmse=0.082797\n",
            "Epoch 025 | train_loss=0.026032 train_rmse=0.161345 | val_loss=0.007456 val_rmse=0.086351\n",
            "Epoch 026 | train_loss=0.025200 train_rmse=0.158744 | val_loss=0.006608 val_rmse=0.081292\n",
            "Epoch 027 | train_loss=0.023059 train_rmse=0.151851 | val_loss=0.006110 val_rmse=0.078167\n",
            "Epoch 028 | train_loss=0.025394 train_rmse=0.159355 | val_loss=0.006207 val_rmse=0.078782\n",
            "Epoch 029 | train_loss=0.022128 train_rmse=0.148756 | val_loss=0.005957 val_rmse=0.077180\n",
            "Epoch 030 | train_loss=0.022833 train_rmse=0.151106 | val_loss=0.005914 val_rmse=0.076902\n",
            "Epoch 031 | train_loss=0.021831 train_rmse=0.147753 | val_loss=0.005987 val_rmse=0.077375\n",
            "Epoch 032 | train_loss=0.022106 train_rmse=0.148680 | val_loss=0.005603 val_rmse=0.074853\n",
            "Epoch 033 | train_loss=0.022214 train_rmse=0.149044 | val_loss=0.004555 val_rmse=0.067494\n",
            "Epoch 034 | train_loss=0.019418 train_rmse=0.139350 | val_loss=0.005286 val_rmse=0.072705\n",
            "Epoch 035 | train_loss=0.021214 train_rmse=0.145649 | val_loss=0.004805 val_rmse=0.069318\n",
            "Epoch 036 | train_loss=0.020666 train_rmse=0.143756 | val_loss=0.004841 val_rmse=0.069580\n",
            "Epoch 037 | train_loss=0.021184 train_rmse=0.145546 | val_loss=0.004647 val_rmse=0.068166\n",
            "Epoch 038 | train_loss=0.020295 train_rmse=0.142461 | val_loss=0.004225 val_rmse=0.064999\n",
            "Epoch 039 | train_loss=0.022134 train_rmse=0.148774 | val_loss=0.004277 val_rmse=0.065397\n",
            "Epoch 040 | train_loss=0.018849 train_rmse=0.137293 | val_loss=0.004198 val_rmse=0.064793\n",
            "Epoch 041 | train_loss=0.018970 train_rmse=0.137733 | val_loss=0.004107 val_rmse=0.064085\n",
            "Epoch 042 | train_loss=0.020462 train_rmse=0.143046 | val_loss=0.004734 val_rmse=0.068805\n",
            "Epoch 043 | train_loss=0.018909 train_rmse=0.137508 | val_loss=0.004691 val_rmse=0.068493\n",
            "Epoch 044 | train_loss=0.018336 train_rmse=0.135412 | val_loss=0.003787 val_rmse=0.061542\n",
            "Epoch 045 | train_loss=0.018716 train_rmse=0.136807 | val_loss=0.003924 val_rmse=0.062639\n",
            "Epoch 046 | train_loss=0.018584 train_rmse=0.136322 | val_loss=0.003562 val_rmse=0.059686\n",
            "Epoch 047 | train_loss=0.018301 train_rmse=0.135281 | val_loss=0.003753 val_rmse=0.061265\n",
            "Epoch 048 | train_loss=0.019702 train_rmse=0.140363 | val_loss=0.003607 val_rmse=0.060060\n",
            "Epoch 049 | train_loss=0.018416 train_rmse=0.135706 | val_loss=0.003305 val_rmse=0.057492\n",
            "Epoch 050 | train_loss=0.018840 train_rmse=0.137258 | val_loss=0.003827 val_rmse=0.061864\n",
            "Epoch 051 | train_loss=0.016326 train_rmse=0.127773 | val_loss=0.003837 val_rmse=0.061941\n",
            "Epoch 052 | train_loss=0.015642 train_rmse=0.125067 | val_loss=0.003262 val_rmse=0.057110\n",
            "Epoch 053 | train_loss=0.015873 train_rmse=0.125988 | val_loss=0.004499 val_rmse=0.067075\n",
            "Epoch 054 | train_loss=0.017380 train_rmse=0.131833 | val_loss=0.003766 val_rmse=0.061366\n",
            "Epoch 055 | train_loss=0.015160 train_rmse=0.123125 | val_loss=0.003271 val_rmse=0.057195\n",
            "Epoch 056 | train_loss=0.016682 train_rmse=0.129158 | val_loss=0.003399 val_rmse=0.058304\n",
            "Epoch 057 | train_loss=0.016024 train_rmse=0.126586 | val_loss=0.003312 val_rmse=0.057550\n",
            "Epoch 058 | train_loss=0.016086 train_rmse=0.126829 | val_loss=0.003958 val_rmse=0.062910\n",
            "Epoch 059 | train_loss=0.015414 train_rmse=0.124154 | val_loss=0.002754 val_rmse=0.052478\n",
            "Epoch 060 | train_loss=0.015052 train_rmse=0.122685 | val_loss=0.003184 val_rmse=0.056424\n",
            "Epoch 061 | train_loss=0.015596 train_rmse=0.124886 | val_loss=0.002517 val_rmse=0.050168\n",
            "Epoch 062 | train_loss=0.015123 train_rmse=0.122974 | val_loss=0.002887 val_rmse=0.053727\n",
            "Epoch 063 | train_loss=0.016567 train_rmse=0.128714 | val_loss=0.002778 val_rmse=0.052702\n",
            "Epoch 064 | train_loss=0.014398 train_rmse=0.119992 | val_loss=0.002968 val_rmse=0.054483\n",
            "Epoch 065 | train_loss=0.015393 train_rmse=0.124068 | val_loss=0.002797 val_rmse=0.052889\n",
            "Epoch 066 | train_loss=0.014286 train_rmse=0.119524 | val_loss=0.002568 val_rmse=0.050677\n",
            "Epoch 067 | train_loss=0.013020 train_rmse=0.114107 | val_loss=0.002503 val_rmse=0.050025\n",
            "Epoch 068 | train_loss=0.012212 train_rmse=0.110506 | val_loss=0.002461 val_rmse=0.049613\n",
            "Epoch 069 | train_loss=0.012981 train_rmse=0.113935 | val_loss=0.002336 val_rmse=0.048328\n",
            "Epoch 070 | train_loss=0.012360 train_rmse=0.111177 | val_loss=0.002713 val_rmse=0.052090\n",
            "Epoch 071 | train_loss=0.013210 train_rmse=0.114935 | val_loss=0.002366 val_rmse=0.048646\n",
            "Epoch 072 | train_loss=0.014454 train_rmse=0.120227 | val_loss=0.002196 val_rmse=0.046862\n",
            "Epoch 073 | train_loss=0.012953 train_rmse=0.113813 | val_loss=0.002345 val_rmse=0.048423\n",
            "Epoch 074 | train_loss=0.015293 train_rmse=0.123665 | val_loss=0.002651 val_rmse=0.051492\n",
            "Epoch 075 | train_loss=0.013498 train_rmse=0.116182 | val_loss=0.002732 val_rmse=0.052268\n",
            "Epoch 076 | train_loss=0.013397 train_rmse=0.115744 | val_loss=0.002531 val_rmse=0.050309\n",
            "Epoch 077 | train_loss=0.012370 train_rmse=0.111219 | val_loss=0.002639 val_rmse=0.051376\n",
            "Epoch 078 | train_loss=0.014195 train_rmse=0.119143 | val_loss=0.002023 val_rmse=0.044977\n",
            "Epoch 079 | train_loss=0.012015 train_rmse=0.109613 | val_loss=0.002307 val_rmse=0.048027\n",
            "Epoch 080 | train_loss=0.012858 train_rmse=0.113392 | val_loss=0.002952 val_rmse=0.054331\n",
            "Epoch 081 | train_loss=0.014031 train_rmse=0.118453 | val_loss=0.001852 val_rmse=0.043037\n",
            "Epoch 082 | train_loss=0.012614 train_rmse=0.112310 | val_loss=0.002211 val_rmse=0.047024\n",
            "Epoch 083 | train_loss=0.012591 train_rmse=0.112208 | val_loss=0.002038 val_rmse=0.045140\n",
            "Epoch 084 | train_loss=0.012026 train_rmse=0.109664 | val_loss=0.002145 val_rmse=0.046314\n",
            "Epoch 085 | train_loss=0.011804 train_rmse=0.108647 | val_loss=0.002771 val_rmse=0.052643\n",
            "Epoch 086 | train_loss=0.012024 train_rmse=0.109654 | val_loss=0.001800 val_rmse=0.042432\n",
            "Epoch 087 | train_loss=0.013379 train_rmse=0.115668 | val_loss=0.002137 val_rmse=0.046226\n",
            "Epoch 088 | train_loss=0.011198 train_rmse=0.105819 | val_loss=0.002351 val_rmse=0.048483\n",
            "Epoch 089 | train_loss=0.010833 train_rmse=0.104081 | val_loss=0.002089 val_rmse=0.045710\n",
            "Epoch 090 | train_loss=0.011647 train_rmse=0.107923 | val_loss=0.002136 val_rmse=0.046216\n",
            "Epoch 091 | train_loss=0.011152 train_rmse=0.105604 | val_loss=0.002112 val_rmse=0.045952\n",
            "Epoch 092 | train_loss=0.011641 train_rmse=0.107893 | val_loss=0.002244 val_rmse=0.047369\n",
            "Epoch 093 | train_loss=0.011456 train_rmse=0.107032 | val_loss=0.003346 val_rmse=0.057843\n",
            "Epoch 094 | train_loss=0.012300 train_rmse=0.110903 | val_loss=0.002051 val_rmse=0.045292\n",
            "Epoch 095 | train_loss=0.011530 train_rmse=0.107379 | val_loss=0.001868 val_rmse=0.043222\n",
            "Epoch 096 | train_loss=0.011152 train_rmse=0.105603 | val_loss=0.001741 val_rmse=0.041730\n",
            "Epoch 097 | train_loss=0.011158 train_rmse=0.105630 | val_loss=0.001502 val_rmse=0.038758\n",
            "Epoch 098 | train_loss=0.011155 train_rmse=0.105617 | val_loss=0.001793 val_rmse=0.042347\n",
            "Epoch 099 | train_loss=0.010754 train_rmse=0.103703 | val_loss=0.002721 val_rmse=0.052162\n",
            "Epoch 100 | train_loss=0.011961 train_rmse=0.109364 | val_loss=0.001567 val_rmse=0.039580\n",
            "Epoch 101 | train_loss=0.013034 train_rmse=0.114168 | val_loss=0.002181 val_rmse=0.046699\n",
            "Epoch 102 | train_loss=0.012710 train_rmse=0.112738 | val_loss=0.002282 val_rmse=0.047770\n",
            "Epoch 103 | train_loss=0.011040 train_rmse=0.105071 | val_loss=0.001767 val_rmse=0.042035\n",
            "Epoch 104 | train_loss=0.011440 train_rmse=0.106959 | val_loss=0.001979 val_rmse=0.044484\n",
            "Epoch 105 | train_loss=0.011236 train_rmse=0.106001 | val_loss=0.001683 val_rmse=0.041023\n",
            "Epoch 106 | train_loss=0.011777 train_rmse=0.108523 | val_loss=0.001945 val_rmse=0.044098\n",
            "Epoch 107 | train_loss=0.011756 train_rmse=0.108427 | val_loss=0.001593 val_rmse=0.039909\n",
            "Epoch 108 | train_loss=0.011406 train_rmse=0.106799 | val_loss=0.001540 val_rmse=0.039247\n",
            "Epoch 109 | train_loss=0.011325 train_rmse=0.106420 | val_loss=0.002455 val_rmse=0.049553\n",
            "Epoch 110 | train_loss=0.010643 train_rmse=0.103167 | val_loss=0.001555 val_rmse=0.039428\n",
            "Epoch 111 | train_loss=0.011051 train_rmse=0.105123 | val_loss=0.002512 val_rmse=0.050117\n",
            "Epoch 112 | train_loss=0.009808 train_rmse=0.099035 | val_loss=0.001469 val_rmse=0.038328\n",
            "Epoch 113 | train_loss=0.009784 train_rmse=0.098915 | val_loss=0.002217 val_rmse=0.047083\n",
            "Epoch 114 | train_loss=0.010241 train_rmse=0.101199 | val_loss=0.001617 val_rmse=0.040218\n",
            "Epoch 115 | train_loss=0.011757 train_rmse=0.108432 | val_loss=0.001805 val_rmse=0.042491\n",
            "Epoch 116 | train_loss=0.010848 train_rmse=0.104152 | val_loss=0.002107 val_rmse=0.045899\n",
            "Epoch 117 | train_loss=0.011196 train_rmse=0.105810 | val_loss=0.002064 val_rmse=0.045434\n",
            "Epoch 118 | train_loss=0.009286 train_rmse=0.096365 | val_loss=0.002094 val_rmse=0.045761\n",
            "Epoch 119 | train_loss=0.011417 train_rmse=0.106849 | val_loss=0.001339 val_rmse=0.036596\n",
            "Epoch 120 | train_loss=0.009908 train_rmse=0.099537 | val_loss=0.001407 val_rmse=0.037509\n",
            "Epoch 121 | train_loss=0.010196 train_rmse=0.100978 | val_loss=0.001456 val_rmse=0.038159\n",
            "Epoch 122 | train_loss=0.010746 train_rmse=0.103664 | val_loss=0.001519 val_rmse=0.038976\n",
            "Epoch 123 | train_loss=0.009596 train_rmse=0.097959 | val_loss=0.001877 val_rmse=0.043321\n",
            "Epoch 124 | train_loss=0.010265 train_rmse=0.101317 | val_loss=0.001626 val_rmse=0.040329\n",
            "Epoch 125 | train_loss=0.008788 train_rmse=0.093745 | val_loss=0.001339 val_rmse=0.036589\n",
            "Epoch 126 | train_loss=0.009916 train_rmse=0.099579 | val_loss=0.001712 val_rmse=0.041376\n",
            "Epoch 127 | train_loss=0.010097 train_rmse=0.100484 | val_loss=0.001338 val_rmse=0.036575\n",
            "Epoch 128 | train_loss=0.010868 train_rmse=0.104249 | val_loss=0.001604 val_rmse=0.040053\n",
            "Epoch 129 | train_loss=0.012644 train_rmse=0.112446 | val_loss=0.001965 val_rmse=0.044331\n",
            "Epoch 130 | train_loss=0.010067 train_rmse=0.100336 | val_loss=0.001515 val_rmse=0.038926\n",
            "Epoch 131 | train_loss=0.009279 train_rmse=0.096328 | val_loss=0.001524 val_rmse=0.039034\n",
            "Epoch 132 | train_loss=0.009877 train_rmse=0.099384 | val_loss=0.001431 val_rmse=0.037832\n",
            "Epoch 133 | train_loss=0.009017 train_rmse=0.094958 | val_loss=0.001687 val_rmse=0.041073\n",
            "Epoch 134 | train_loss=0.009501 train_rmse=0.097472 | val_loss=0.001749 val_rmse=0.041826\n",
            "Epoch 135 | train_loss=0.009992 train_rmse=0.099961 | val_loss=0.001652 val_rmse=0.040641\n",
            "Epoch 136 | train_loss=0.009023 train_rmse=0.094990 | val_loss=0.001329 val_rmse=0.036458\n",
            "Epoch 137 | train_loss=0.010119 train_rmse=0.100592 | val_loss=0.001579 val_rmse=0.039741\n",
            "Epoch 138 | train_loss=0.009211 train_rmse=0.095975 | val_loss=0.001474 val_rmse=0.038389\n",
            "Epoch 139 | train_loss=0.008858 train_rmse=0.094119 | val_loss=0.001738 val_rmse=0.041688\n",
            "Epoch 140 | train_loss=0.010257 train_rmse=0.101275 | val_loss=0.001470 val_rmse=0.038338\n",
            "Epoch 141 | train_loss=0.009918 train_rmse=0.099590 | val_loss=0.001659 val_rmse=0.040730\n",
            "Epoch 142 | train_loss=0.011097 train_rmse=0.105341 | val_loss=0.002083 val_rmse=0.045644\n",
            "Epoch 143 | train_loss=0.010379 train_rmse=0.101878 | val_loss=0.001754 val_rmse=0.041875\n",
            "Epoch 144 | train_loss=0.009279 train_rmse=0.096328 | val_loss=0.001275 val_rmse=0.035705\n",
            "Epoch 145 | train_loss=0.008536 train_rmse=0.092388 | val_loss=0.002066 val_rmse=0.045458\n",
            "Epoch 146 | train_loss=0.009025 train_rmse=0.094999 | val_loss=0.001538 val_rmse=0.039222\n",
            "Epoch 147 | train_loss=0.009625 train_rmse=0.098105 | val_loss=0.001305 val_rmse=0.036126\n",
            "Epoch 148 | train_loss=0.008990 train_rmse=0.094816 | val_loss=0.001625 val_rmse=0.040306\n",
            "Epoch 149 | train_loss=0.007839 train_rmse=0.088538 | val_loss=0.001147 val_rmse=0.033866\n",
            "Epoch 150 | train_loss=0.007647 train_rmse=0.087449 | val_loss=0.001972 val_rmse=0.044402\n",
            "Epoch 151 | train_loss=0.008295 train_rmse=0.091077 | val_loss=0.001361 val_rmse=0.036887\n",
            "Epoch 152 | train_loss=0.009441 train_rmse=0.097164 | val_loss=0.001606 val_rmse=0.040081\n",
            "Epoch 153 | train_loss=0.009088 train_rmse=0.095333 | val_loss=0.001474 val_rmse=0.038393\n",
            "Epoch 154 | train_loss=0.010388 train_rmse=0.101921 | val_loss=0.002110 val_rmse=0.045936\n",
            "Epoch 155 | train_loss=0.008412 train_rmse=0.091719 | val_loss=0.002001 val_rmse=0.044736\n",
            "Epoch 156 | train_loss=0.009501 train_rmse=0.097473 | val_loss=0.001295 val_rmse=0.035990\n",
            "Epoch 157 | train_loss=0.009771 train_rmse=0.098847 | val_loss=0.001342 val_rmse=0.036628\n",
            "Epoch 158 | train_loss=0.009531 train_rmse=0.097625 | val_loss=0.001270 val_rmse=0.035642\n",
            "Epoch 159 | train_loss=0.010193 train_rmse=0.100960 | val_loss=0.001394 val_rmse=0.037336\n",
            "Epoch 160 | train_loss=0.007997 train_rmse=0.089427 | val_loss=0.001364 val_rmse=0.036931\n",
            "Epoch 161 | train_loss=0.009855 train_rmse=0.099272 | val_loss=0.001586 val_rmse=0.039829\n",
            "Epoch 162 | train_loss=0.009260 train_rmse=0.096227 | val_loss=0.002084 val_rmse=0.045647\n",
            "Epoch 163 | train_loss=0.008856 train_rmse=0.094108 | val_loss=0.001484 val_rmse=0.038529\n",
            "Epoch 164 | train_loss=0.008730 train_rmse=0.093437 | val_loss=0.002382 val_rmse=0.048801\n",
            "Epoch 165 | train_loss=0.008478 train_rmse=0.092078 | val_loss=0.001388 val_rmse=0.037253\n",
            "Epoch 166 | train_loss=0.009920 train_rmse=0.099599 | val_loss=0.001493 val_rmse=0.038635\n",
            "Epoch 167 | train_loss=0.008392 train_rmse=0.091609 | val_loss=0.001408 val_rmse=0.037523\n",
            "Epoch 168 | train_loss=0.008351 train_rmse=0.091386 | val_loss=0.001478 val_rmse=0.038441\n",
            "Epoch 169 | train_loss=0.009442 train_rmse=0.097170 | val_loss=0.001224 val_rmse=0.034984\n",
            "Epoch 170 | train_loss=0.008889 train_rmse=0.094281 | val_loss=0.001575 val_rmse=0.039691\n",
            "Early stopping at epoch 170. Best val_loss: 0.001147\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate"
      ],
      "metadata": {
        "id": "krKUjYJq19ob"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    test_loss, test_rmse = run_epoch(test_loader, model, optimizer=None)\n",
        "print(f\"Eval (scaled y): loss={test_loss:.6f}, rmse={test_rmse:.6f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VVy9MXKL1-yv",
        "outputId": "2eafe6f5-508e-4f81-e371-36eee2a41978"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval (scaled y): loss=0.001187, rmse=0.034447\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "y_pred_s_list = []\n",
        "with torch.no_grad():\n",
        "    for xb, _ in test_loader:\n",
        "        xb = xb.to(device)\n",
        "        preds = model(xb).cpu().numpy()\n",
        "        y_pred_s_list.append(preds)\n",
        "\n",
        "y_pred_s = np.vstack(y_pred_s_list)           # scaled predictions\n",
        "y_pred = scaler_y.inverse_transform(y_pred_s) # back to original scale\n",
        "\n",
        "r2   = r2_score(y_test, y_pred)  # (y_true, y_pred)\n",
        "rmse = np.sqrt(np.mean((y_pred - y_test) ** 2))\n",
        "mae  = np.mean(np.abs(y_pred - y_test))\n",
        "\n",
        "print(f\"Test R2 (original scale):   {r2:.4f}\")\n",
        "print(f\"Test RMSE (original scale): {rmse:.4f}\")\n",
        "print(f\"Test MAE  (original scale): {mae:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fJN8Tf6w2EEb",
        "outputId": "a51a91ac-d369-42ce-99ba-e0a81697e585"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test R2 (original scale):   0.9989\n",
            "Test RMSE (original scale): 4324.0210\n",
            "Test MAE  (original scale): 3587.2085\n"
          ]
        }
      ]
    }
  ]
}